{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFUdPMpq-bWR"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfytUYAj4rMB"
   },
   "outputs": [],
   "source": [
    "# import gdown\n",
    "\n",
    "# files = {\n",
    "#     'F01.7z': \"https://drive.google.com/uc?id=0B4PVUqnGmrJsMXJIekMzMm1ZTzg&export=download\",\n",
    "#     \"F02.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsWnFvZkp5dm5jRTQ&export=download\",\n",
    "#     \"F04.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsZ1N3azVMa2hFdVE&export=download\",\n",
    "#     \"F05.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsRnN0ZTBGYm91dk0&export=download\",\n",
    "#     \"F06.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsTHZhS0RubnRmRXc&export=download\",\n",
    "#     \"F07.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsdzBVWUwyLU1fNFE&export=download\",\n",
    "#     \"F08.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsQVN5ZUpvbkRjRE0&export=download\",\n",
    "#     \"F09.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJscU5nT2otMjdpbTQ&export=download\",\n",
    "#     \"F10.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsM0tHdi10azVvNDQ&export=download\",\n",
    "#     \"F11.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsaGtldzY4aWFJOGs&export=download\",\n",
    "#     \"M01.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsZHhNZ09UbmE2Rm8&export=download\",\n",
    "#     \"M02.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsM18tOGVEV1d3b00&export=download\",\n",
    "#     \"M04.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsdGQyaGRkb0E2UXM&export=download\",\n",
    "#     \"M07.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsc204ak0yMVBTUkU&export=download\",\n",
    "#     \"M08.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsc2Fyb3BiMm9Rd2c&export=download\"\n",
    "#   }\n",
    "\n",
    "# for key in files:\n",
    "#     gdown.download(files[key], key, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e04cIknA430K"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p data/miracl\n",
    "\n",
    "# for key in files:\n",
    "#     file = key\n",
    "#     dest = 'data/miracl/' + key.split('.')[0]\n",
    "#     !7z x $key -o$dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-1z7YV8-nh4"
   },
   "source": [
    "### Import. Setup dirs, classes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imutils.video import VideoStream\n",
    "#from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "#import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import shutil \n",
    "import math\n",
    "import numpy as np\n",
    "#from albumentations.augmentations import transforms\n",
    "#import skimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from skimage.transform import resize\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from scipy import stats as s\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_black = np.zeros((70, 140, 3))\n",
    "\n",
    "base_dir = 'data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "classes = 'Begin, Choose, Connection, Navigation, Next, Previous, Start, Stop, Hello, Web'\n",
    "classes = classes.split(', ')\n",
    "\n",
    "#predictor_path = 'assests/predictors/shape_predictor_68_face_landmarks.dat'\n",
    "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "#train_people = 'F01, F02, F04, F05, F06, F07, F08, F09, M01, M02, M04'.split(', ')\n",
    "train_people = 'F01, F02, F04, F05, F06, F07, F08, F09, M01, M02, M04, F11, M08'.split(', ')\n",
    "val_people = 'F10, M07'.split(', ')\n",
    "#test_people = 'F11, M08'.split(', ')\n",
    "print(train_people)\n",
    "print(val_people)\n",
    "#print(test_people)\n",
    "\n",
    "classes_num = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
    "word_ids = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
    "classes_dict = dict(zip(classes_num, classes))\n",
    "print(classes_num)\n",
    "print(word_ids)\n",
    "print(classes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/train\n",
    "!rm -rf data/validation\n",
    "!rm -rf data/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in classes:\n",
    "    train_vids_dir = os.path.join(train_dir, class_name)\n",
    "    val_vids_dir = os.path.join(val_dir, class_name)\n",
    "    test_vids_dir = os.path.join(test_dir, class_name)\n",
    "    \n",
    "    os.makedirs(train_vids_dir)\n",
    "    os.makedirs(val_vids_dir)\n",
    "    os.makedirs(test_vids_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtTNdaei-Sua"
   },
   "source": [
    "### Extract frames, crop around mouth (train and val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for classi in tqdm(classes_num[:]):\n",
    "    for person in train_people[:]:\n",
    "        for word_id in word_ids[:]:\n",
    "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "                img = cv2.imread(f, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray)\n",
    "                \n",
    "                for k, rect in enumerate(rects):\n",
    "                    shape = predictor(gray, rect)\n",
    "                    \n",
    "                    x_51 = shape.part(51).x\n",
    "                    y_51 = shape.part(51).y\n",
    "                    x_57 = shape.part(57).x\n",
    "                    y_57 = shape.part(57).y\n",
    "\n",
    "                    x1_m = x_51 - 18\n",
    "                    y1_m = y_51 - 7\n",
    "                    x2_m = x_57 + 18\n",
    "                    y2_m = y_57 + 9\n",
    "\n",
    "                    offset_x_m = (70-(abs(x1_m-x2_m)))/2\n",
    "                    offset_y_m = (35-(abs(y1_m-y2_m)))/2\n",
    "\n",
    "                    img = img[int(y1_m-offset_y_m):int(y2_m+offset_y_m), int(x1_m-offset_x_m):int(x2_m+offset_x_m)]\n",
    "                    \n",
    "                    scale_percent = 200\n",
    "                    width = int(img.shape[1] * scale_percent / 100)\n",
    "                    height = int(img.shape[0] * scale_percent / 100)\n",
    "                    dim = (width, height)\n",
    "                    img = cv2.resize(img, (int(img.shape[1]*200/100), int(img.shape[0]*200/100)), interpolation=cv2.INTER_AREA) \n",
    "                    \n",
    "                counter += 1\n",
    "                \n",
    "                cv2.imwrite('data/train/' + classes_dict[classi] + '/' + classi  + '_' + person  + '_' + word_id + '_' + f[28:-4] + '.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for classi in tqdm(classes_num[:]):\n",
    "    for person in val_people[:]:\n",
    "        for word_id in word_ids[:]:\n",
    "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "                img = cv2.imread(f, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray)\n",
    "                \n",
    "                for k, rect in enumerate(rects):\n",
    "                    shape = predictor(gray, rect)\n",
    "                    \n",
    "                    x_51 = shape.part(51).x\n",
    "                    y_51 = shape.part(51).y\n",
    "                    x_57 = shape.part(57).x\n",
    "                    y_57 = shape.part(57).y\n",
    "\n",
    "                    x1_m = x_51 - 18\n",
    "                    y1_m = y_51 - 7\n",
    "                    x2_m = x_57 + 18\n",
    "                    y2_m = y_57 + 9\n",
    "\n",
    "                    offset_x_m = (70-(abs(x1_m-x2_m)))/2\n",
    "                    offset_y_m = (35-(abs(y1_m-y2_m)))/2\n",
    "\n",
    "                    img = img[int(y1_m-offset_y_m):int(y2_m+offset_y_m), int(x1_m-offset_x_m):int(x2_m+offset_x_m)]\n",
    "                    \n",
    "                    scale_percent = 200\n",
    "                    width = int(img.shape[1] * scale_percent / 100)\n",
    "                    height = int(img.shape[0] * scale_percent / 100)\n",
    "                    dim = (width, height)\n",
    "                    img = cv2.resize(img, (int(img.shape[1]*200/100), int(img.shape[0]*200/100)), interpolation=cv2.INTER_AREA) \n",
    "                    \n",
    "                counter += 1\n",
    "                \n",
    "                cv2.imwrite('data/validation/' + classes_dict[classi] + '/' + classi  + '_' + person  + '_' + word_id + '_' + f[28:-4] + '.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkiSwKjkGugn"
   },
   "source": [
    "## Get distribution of numbers of frames across videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnopPrpIG65W"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = [] # ['data/train/Begin/01_F01_01_color_001.jpg', 'data/train/Begin/01_F01_01_color_002.jpg', 'data/train/Begin/01_F01_01_color_003.jpg' ... ]             12109\n",
    "video_names = [] # ['01_F01_01', '01_F01_01', '01_F01_01' ... ]             12109\n",
    "frame_nums = [] # ['001', '002', '003' ... ]            12109\n",
    "video_names_uniq = [] # ['01_F01_01', '01_F01_02', '01_F01_03' ... ]            1100\n",
    "frame_nums_uniq = [] # ['010', '007', '010' ... ]           1100\n",
    "vids_and_frames = {} # {'01_F01_01': '010', '01_F01_02': '007', '01_F01_03': '010' ... }            1100\n",
    "frames_distribution = {} # {'005': 3, '006': 20, '007': 69 ... '022': 1}            18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images.append(i)\n",
    "        \n",
    "for img in all_images:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums.append(frame_num)\n",
    "    \n",
    "video_names_uniq = list(sorted(set(video_names)))\n",
    "\n",
    "for i in range(len(video_names)):\n",
    "    if i < len(video_names)-1:\n",
    "        if video_names[i] == video_names[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq.append(frame_nums[i])\n",
    "    else:\n",
    "        frame_nums_uniq.append(frame_nums[-1])\n",
    "        \n",
    "for i in range(len(video_names_uniq)):\n",
    "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
    "    \n",
    "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
    "    count_d = 0\n",
    "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
    "        if frames_num == frame_num_uniq:\n",
    "            count_d += 1\n",
    "            frames_distribution[frame_num_uniq] = count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35sUrHQjI4mX"
   },
   "source": [
    "**Val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_val = [] # 1746\n",
    "video_names_val = [] # 1746\n",
    "frame_nums_val = [] # 1746\n",
    "video_names_uniq_val = [] # 200\n",
    "frame_nums_uniq_val = [] # 200\n",
    "vids_and_frames_val = {} # 200\n",
    "frames_distribution_val = {} # 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
    "        all_images_val.append(i)\n",
    "        \n",
    "for img in all_images_val:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names_val.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums_val.append(frame_num)\n",
    "    \n",
    "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
    "\n",
    "for i in range(len(video_names_val)):\n",
    "    if i < len(video_names_val)-1:\n",
    "        if video_names_val[i] == video_names_val[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
    "    else:\n",
    "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
    "        \n",
    "for i in range(len(frame_nums_uniq_val)):\n",
    "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
    "        \n",
    "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
    "    count_d = 0\n",
    "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
    "        if frames_num_val == frame_num_uniq_val:\n",
    "            count_d += 1\n",
    "            frames_distribution_val[frame_num_uniq_val] = count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ip7S1zh_KsQX"
   },
   "source": [
    "**Plot distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train max frames:', int(max(vids_and_frames.values())))\n",
    "print('Train min frames:', int(min(vids_and_frames.values())))\n",
    "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
    "\n",
    "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
    "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
    "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
    "\n",
    "ax1.bar(list(frames_distribution.keys()), [int(f) for f in frames_distribution.values()], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "ax1.set_yticks(np.arange(0, max(frames_distribution.values())+2, 10))\n",
    "\n",
    "ax2.bar(list(frames_distribution_val.keys()), [int(f) for f in frames_distribution_val.values()], 0.7)\n",
    "ax2.grid(axis = 'y')\n",
    "ax2.set_yticks(np.arange(0, max(frames_distribution_val.values())+2, 10)) \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ry4GW-UcBjbY"
   },
   "source": [
    "**Stats across classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names_in_class_dict = {}\n",
    "idx = 0\n",
    "\n",
    "for classi in classes_dict.values():\n",
    "    video_names_in_class_dict[classi] = {}\n",
    "    video_names_in_class_dict[classi]['num of frames'] = 0\n",
    "    video_names_in_class_dict[classi]['num of vids'] = 0\n",
    "    video_names_in_class_dict[classi]['vids'] = []\n",
    "    video_names_in_class_dict[classi]['frames per vid'] = []\n",
    "    video_names_in_class_dict[classi]['mean frames per vid'] = 0\n",
    "\n",
    "    all_images_in_class = []\n",
    "    video_names = []\n",
    "    video_names_uniq_in_class = []\n",
    "    frame_nums_in_class = []\n",
    "    frame_nums_uniq_in_class = []\n",
    "    vids_and_frames_in_class = {}\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images_in_class.append(i)\n",
    "        counter += 1\n",
    "\n",
    "    video_names_in_class_dict[classi]['num of frames'] = counter\n",
    "\n",
    "    for img in all_images_in_class:\n",
    "        img_name = img.split('/')[-1].split('.')[0]\n",
    "        video_name = img_name[:9]\n",
    "        video_names.append(video_name)\n",
    "        frame_num = img_name[-3:]\n",
    "        frame_nums_in_class.append(frame_num)\n",
    "\n",
    "    video_names_uniq_in_class = list(sorted(set(video_names)))\n",
    "\n",
    "    video_names_in_class_dict[classi]['vids'] = video_names_uniq_in_class\n",
    "    video_names_in_class_dict[classi]['num of vids'] = len(video_names_uniq_in_class)\n",
    "\n",
    "    for i in range(len(video_names)):\n",
    "        if i < len(video_names)-1:\n",
    "            if video_names[i] == video_names[i+1]:\n",
    "                pass\n",
    "            else:\n",
    "                frame_nums_uniq_in_class.append(frame_nums_in_class[i])\n",
    "        else:\n",
    "            frame_nums_uniq_in_class.append(frame_nums_in_class[-1])\n",
    "\n",
    "    for i in range(len(video_names_uniq_in_class)):\n",
    "        vids_and_frames_in_class[video_names_uniq_in_class[i]] = frame_nums_uniq_in_class[i]\n",
    "\n",
    "    video_names_in_class_dict[classi]['frames per vid'] = list(vids_and_frames_in_class.values())\n",
    "    video_names_in_class_dict[classi]['mean frames per vid'] = np.array(list(list(video_names_in_class_dict.values())[idx].values())[3]).astype(np.float).mean()\n",
    "\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График слева - сколько всего фреймов в видосах у каждого класса + количество видосов. Изначально их по 130 у каждого класса\n",
    "# График справа - среднее число кадров в видео у каждого класса\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
    "\n",
    "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "ax1.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))]) + 2, 150))\n",
    "\n",
    "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "#ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]) + 2, 150))\n",
    "\n",
    "ax2.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))], 0.7)\n",
    "ax2.grid(axis = 'y')\n",
    "ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))]) + 2, 1))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B4mWFtw5vdq"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import google\n",
    "\n",
    "# with open('video_names_in_class.json', 'w') as jfile:\n",
    "#     json.dump(video_names_in_class, jfile)\n",
    "\n",
    "# google.colab.files.download('video_names_in_class.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGO1WZN4LsZ-"
   },
   "source": [
    "## Delete videos either too short or too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubYd-5qXL3EN"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = {}\n",
    "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
    "    comp[frame_num_uniq] = {}\n",
    "    comp[frame_num_uniq]['class'] = []\n",
    "    comp[frame_num_uniq]['person'] = []\n",
    "    comp[frame_num_uniq]['video_names'] = []\n",
    "    comp[frame_num_uniq]['videos_count'] = 0\n",
    "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
    "        if frames_num == frame_num_uniq:\n",
    "            if vid_name.split('_')[0] not in comp[frame_num_uniq]['class']:\n",
    "                comp[frame_num_uniq]['class'].append(vid_name.split('_')[0])\n",
    "            if vid_name.split('_')[1] not in comp[frame_num_uniq]['person']:\n",
    "                comp[frame_num_uniq]['person'].append(vid_name.split('_')[1])\n",
    "            if vid_name not in comp[frame_num_uniq]['video_names']:\n",
    "                comp[frame_num_uniq]['video_names'].append(vid_name)\n",
    "                comp[frame_num_uniq]['videos_count'] += 1\n",
    "\n",
    "# get the most common number of frames\n",
    "maxer = 0\n",
    "maxer_key = 0\n",
    "for key in frames_distribution:\n",
    "    if frames_distribution[key] >= maxer:\n",
    "        maxer = frames_distribution[key]\n",
    "        maxer_key = key\n",
    "\n",
    "# videos either too short or too long (with num. of frames not in [7, 8, 9, 10, 11, 12, 13])\n",
    "# total to 233 vids\n",
    "abnormal_vids = []\n",
    "for key in comp:\n",
    "    for video_name in comp[key]['video_names']:\n",
    "        if int(key) not in range(int(maxer_key)-3, int(maxer_key)+4):\n",
    "            abnormal_vids.append(video_name)\n",
    "abnormal_vids = sorted(abnormal_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(int(maxer_key)-3, int(maxer_key)+4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove not needed\n",
    "# debug note: the last print must return \"Removed 233 videos\" - the number of videos to remove\n",
    "\n",
    "counter = 0\n",
    "added = 0\n",
    "matched = []\n",
    "for classi in list(classes_dict.values()):\n",
    "    removed = 0\n",
    "    for abnormal_vid in abnormal_vids:\n",
    "        for vid in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "            if abnormal_vid == vid.split('/')[-1].split('.')[0].split('_color')[0]:\n",
    "                os.remove(vid)\n",
    "                #!rm $vid\n",
    "                matched.append(abnormal_vid)\n",
    "                counter += 1\n",
    "                removed += 1\n",
    "    print(classi, 'Removed', counter, '(' , removed, 'in this dir)')\n",
    "\n",
    "print()\n",
    "print('Removed', len(set(matched)), 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hk13BEvCMqet"
   },
   "outputs": [],
   "source": [
    "# optionally, save the distribution as a json and visualize\n",
    "# https://codebeautify.org/jsonviewer\n",
    "\n",
    "# import json\n",
    "# from google.colab import files\n",
    "\n",
    "# with open('comp.json', 'w') as jfile:\n",
    "#     json.dump(comp, jfile)\n",
    "\n",
    "# files.download('comp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_val = {}\n",
    "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
    "    comp_val[frame_num_uniq_val] = {}\n",
    "    comp_val[frame_num_uniq_val]['class'] = []\n",
    "    comp_val[frame_num_uniq_val]['person'] = []\n",
    "    comp_val[frame_num_uniq_val]['video_names'] = []\n",
    "    comp_val[frame_num_uniq_val]['videos_count'] = 0\n",
    "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
    "        if frames_num_val == frame_num_uniq_val:\n",
    "            if vid_name_val.split('_')[0] not in comp_val[frame_num_uniq_val]['class']:\n",
    "                comp_val[frame_num_uniq_val]['class'].append(vid_name_val.split('_')[0])\n",
    "            if vid_name_val.split('_')[1] not in comp_val[frame_num_uniq_val]['person']:\n",
    "                comp_val[frame_num_uniq_val]['person'].append(vid_name_val.split('_')[1])\n",
    "            if vid_name_val not in comp_val[frame_num_uniq_val]['video_names']:\n",
    "                comp_val[frame_num_uniq_val]['video_names'].append(vid_name_val)\n",
    "                comp_val[frame_num_uniq_val]['videos_count'] += 1\n",
    "\n",
    "maxer = 0\n",
    "maxer_key = 0\n",
    "for key in frames_distribution_val:\n",
    "    if frames_distribution_val[key] >= maxer:\n",
    "        maxer = frames_distribution_val[key]\n",
    "        maxer_key = key\n",
    "\n",
    "# videos either too short or too long (with num. of frames not in [6, 7, 8, 9, 10])\n",
    "# total to 35 vids\n",
    "abnormal_vids = []\n",
    "for key in comp_val:\n",
    "    for video_name in comp_val[key]['video_names']:\n",
    "        if int(key) not in range(int(maxer_key)-1, int(maxer_key)+6):\n",
    "            abnormal_vids.append(video_name)\n",
    "abnormal_vids = sorted(abnormal_vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(int(maxer_key)-1, int(maxer_key)+6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug note: the past print must return \"Removed 35 videos\" - the number of videos to remove\n",
    "counter = 0\n",
    "added = 0\n",
    "matched = []\n",
    "for classi in list(classes_dict.values()):\n",
    "    removed = 0\n",
    "    for abnormal_vid in abnormal_vids:\n",
    "        for vid in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
    "            if abnormal_vid == vid.split('/')[-1].split('.')[0].split('_color')[0]:\n",
    "                os.remove(vid)\n",
    "                matched.append(abnormal_vid)\n",
    "                counter += 1\n",
    "                removed += 1\n",
    "    print(classi, 'Removed', counter, '(' , removed, 'in this dir)')\n",
    "\n",
    "print()\n",
    "print('Removed', len(set(matched)), 'videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-Cwx0A6P5Cc"
   },
   "outputs": [],
   "source": [
    "# optionally, save the distribution as a json and visualize\n",
    "# https://codebeautify.org/jsonviewer\n",
    "\n",
    "# import json\n",
    "# from google.colab import files\n",
    "\n",
    "# with open('comp_val.json', 'w') as jfile:\n",
    "#     json.dump(comp_val, jfile)\n",
    "\n",
    "# files.download('comp_val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnS97OtiWdHM"
   },
   "source": [
    "## Check the distribution again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IoNyVjBWdHR"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = [] # 8699 vs 12109\n",
    "video_names = [] # 8699 vs 12109\n",
    "frame_nums = [] # 8699 vs 12109\n",
    "video_names_uniq = [] # 867 vs 1100\n",
    "frame_nums_uniq = [] # 867 vs 1100\n",
    "vids_and_frames = {} # 867 vs 1100\n",
    "frames_distribution = {} # 7 vs 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images.append(i)\n",
    "        \n",
    "for img in all_images:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums.append(frame_num)\n",
    "    \n",
    "video_names_uniq = list(sorted(set(video_names)))\n",
    "\n",
    "for i in range(len(video_names)):\n",
    "    if i < len(video_names)-1:\n",
    "        if video_names[i] == video_names[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq.append(frame_nums[i])\n",
    "    else:\n",
    "        frame_nums_uniq.append(frame_nums[-1])\n",
    "        \n",
    "for i in range(len(video_names_uniq)):\n",
    "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
    "    \n",
    "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
    "    count_d = 0\n",
    "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
    "        if frames_num == frame_num_uniq:\n",
    "            count_d += 1\n",
    "            frames_distribution[frame_num_uniq] = count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVUWOj6-WdHa"
   },
   "source": [
    "**Val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_val = [] # 1364 vs 1746\n",
    "video_names_val = [] # 1364 vs 1746\n",
    "frame_nums_val = [] # 1364 vs 1746\n",
    "video_names_uniq_val = [] # 165 vs 200\n",
    "frame_nums_uniq_val = [] # 165 vs 200\n",
    "vids_and_frames_val = {} # 165 vs 200\n",
    "frames_distribution_val = {} # 5 vs 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
    "        all_images_val.append(i)\n",
    "        \n",
    "for img in all_images_val:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names_val.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums_val.append(frame_num)\n",
    "    \n",
    "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
    "\n",
    "for i in range(len(video_names_val)):\n",
    "    if i < len(video_names_val)-1:\n",
    "        if video_names_val[i] == video_names_val[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
    "    else:\n",
    "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
    "        \n",
    "for i in range(len(frame_nums_uniq_val)):\n",
    "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
    "        \n",
    "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
    "    count_d = 0\n",
    "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
    "        if frames_num_val == frame_num_uniq_val:\n",
    "            count_d += 1\n",
    "            frames_distribution_val[frame_num_uniq_val] = count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiUpwWRmWdHj"
   },
   "source": [
    "**Plot distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train max frames:', int(max(vids_and_frames.values())))\n",
    "print('Train min frames:', int(min(vids_and_frames.values())))\n",
    "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
    "\n",
    "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
    "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
    "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
    "\n",
    "ax1.bar(list(frames_distribution.keys()), [int(f) for f in frames_distribution.values()], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "ax1.set_yticks(np.arange(0, max(frames_distribution.values())+2, 10))\n",
    "\n",
    "ax2.bar(list(frames_distribution_val.keys()), [int(f) for f in frames_distribution_val.values()], 0.7)\n",
    "ax2.grid(axis = 'y')\n",
    "ax2.set_yticks(np.arange(0, max(frames_distribution_val.values())+2, 10)) \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qsj6Mh0KB0UV"
   },
   "source": [
    "**Stats across classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names_in_class_dict = {}\n",
    "idx = 0\n",
    "\n",
    "for classi in classes_dict.values():\n",
    "    video_names_in_class_dict[classi] = {}\n",
    "    video_names_in_class_dict[classi]['num of frames'] = 0\n",
    "    video_names_in_class_dict[classi]['num of vids'] = 0\n",
    "    video_names_in_class_dict[classi]['vids'] = []\n",
    "    video_names_in_class_dict[classi]['frames per vid'] = []\n",
    "    video_names_in_class_dict[classi]['mean frames per vid'] = 0\n",
    "\n",
    "    all_images_in_class = []\n",
    "    video_names = []\n",
    "    video_names_uniq_in_class = []\n",
    "    frame_nums_in_class = []\n",
    "    frame_nums_uniq_in_class = []\n",
    "    vids_and_frames_in_class = {}\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images_in_class.append(i)\n",
    "        counter += 1\n",
    "\n",
    "    video_names_in_class_dict[classi]['num of frames'] = counter\n",
    "\n",
    "    for img in all_images_in_class:\n",
    "        img_name = img.split('/')[-1].split('.')[0]\n",
    "        video_name = img_name[:9]\n",
    "        video_names.append(video_name)\n",
    "        frame_num = img_name[-3:]\n",
    "        frame_nums_in_class.append(frame_num)\n",
    "\n",
    "    video_names_uniq_in_class = list(sorted(set(video_names)))\n",
    "\n",
    "    video_names_in_class_dict[classi]['vids'] = video_names_uniq_in_class\n",
    "    video_names_in_class_dict[classi]['num of vids'] = len(video_names_uniq_in_class)\n",
    "\n",
    "    for i in range(len(video_names)):\n",
    "        if i < len(video_names)-1:\n",
    "            if video_names[i] == video_names[i+1]:\n",
    "                pass\n",
    "            else:\n",
    "                frame_nums_uniq_in_class.append(frame_nums_in_class[i])\n",
    "        else:\n",
    "            frame_nums_uniq_in_class.append(frame_nums_in_class[-1])\n",
    "\n",
    "    for i in range(len(video_names_uniq_in_class)):\n",
    "        vids_and_frames_in_class[video_names_uniq_in_class[i]] = frame_nums_uniq_in_class[i]\n",
    "\n",
    "    video_names_in_class_dict[classi]['frames per vid'] = list(vids_and_frames_in_class.values())\n",
    "    video_names_in_class_dict[classi]['mean frames per vid'] = np.array(list(list(video_names_in_class_dict.values())[idx].values())[3]).astype(np.float).mean()\n",
    "\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим просадку у класса Navigation - было 130 видео, стало 80. Число кадров соотевтстенно тоже упало.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
    "\n",
    "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "ax1.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))]) + 2, 150))\n",
    "\n",
    "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))], 0.7)\n",
    "ax1.grid(axis = 'y')\n",
    "#ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]) + 2, 150))\n",
    "\n",
    "ax2.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))], 0.7)\n",
    "ax2.grid(axis = 'y')\n",
    "ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))]) + 2, 1))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgsim5KYZ8WA"
   },
   "source": [
    "## Perform padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that padding's shape equals to (w x h x c) of the train images\n",
    "\n",
    "print(img_black.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame_num = int(max(vids_and_frames.values())) #13\n",
    "target_frame_num_val = int(max(vids_and_frames_val.values())) # 10\n",
    "print(target_frame_num)\n",
    "print(target_frame_num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad train\n",
    "\n",
    "for classi in classes_dict.keys():\n",
    "    for vid in vids_and_frames.keys():\n",
    "        if classi == vid.split('_')[0]:\n",
    "            if int(vids_and_frames[vid]) < target_frame_num:\n",
    "                for i in range(1, (target_frame_num - int(vids_and_frames[vid]) + 1)):\n",
    "                    cv2.imwrite('data/train/' + classes_dict[classi] + '/' + vid + '_' + 'color' + '_' + str(int(vids_and_frames[vid]) + i).zfill(3) + '.jpg', img_black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad val\n",
    "\n",
    "for classi in classes_dict.keys():\n",
    "    for vid in vids_and_frames_val.keys():\n",
    "        if classi == vid.split('_')[0]:\n",
    "            if int(vids_and_frames_val[vid]) < target_frame_num:\n",
    "                for i in range(1, (target_frame_num - int(vids_and_frames_val[vid]) + 1)):\n",
    "                    cv2.imwrite('data/validation/' + classes_dict[classi] + '/' + vid + '_' + 'color' + '_' + str(int(vids_and_frames_val[vid]) + i).zfill(3) + '.jpg', img_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ODHlUJzbKnc"
   },
   "source": [
    "## Check the distribution the last time to be sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtWQZGFYbKnf"
   },
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = [] # 11271 vs 8699 vs 12109\n",
    "video_names = [] # 11271 vs 8699 vs 12109\n",
    "frame_nums = [] # 11271 vs 8699 vs 12109\n",
    "video_names_uniq = [] # 867 vs 867 vs 1100\n",
    "frame_nums_uniq = [] # 867 vs 867 vs 1100\n",
    "vids_and_frames = {} # 867 vs 867 vs 1100\n",
    "frames_distribution = {} # 1 vs 7 vs 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images.append(i)\n",
    "        \n",
    "for img in all_images:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums.append(frame_num)\n",
    "    \n",
    "video_names_uniq = list(sorted(set(video_names)))\n",
    "\n",
    "for i in range(len(video_names)):\n",
    "    if i < len(video_names)-1:\n",
    "        if video_names[i] == video_names[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq.append(frame_nums[i])\n",
    "    else:\n",
    "        frame_nums_uniq.append(frame_nums[-1])\n",
    "        \n",
    "for i in range(len(video_names_uniq)):\n",
    "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
    "    \n",
    "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
    "    count_d = 0\n",
    "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
    "        if frames_num == frame_num_uniq:\n",
    "            count_d += 1\n",
    "            frames_distribution[frame_num_uniq] = count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMhtEhT_bKns"
   },
   "source": [
    "**Val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_val = [] # 1650 vs 1364 vs 1746\n",
    "video_names_val = [] # 1650 vs 1364 vs 1746\n",
    "frame_nums_val = [] # 1650 vs 1364 vs 1746\n",
    "video_names_uniq_val = [] # 165 vs 165 vs 200\n",
    "frame_nums_uniq_val = [] # 165 vs 165 vs 200\n",
    "vids_and_frames_val = {} # 165 vs 165 vs 200\n",
    "frames_distribution_val = {} # 1 vs 5 vs 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
    "        all_images_val.append(i)\n",
    "        \n",
    "for img in all_images_val:\n",
    "    img_name = img.split('/')[-1].split('.')[0]\n",
    "    video_name = img_name[:9]\n",
    "    video_names_val.append(video_name)\n",
    "    frame_num = img_name[-3:]\n",
    "    frame_nums_val.append(frame_num)\n",
    "    \n",
    "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
    "\n",
    "for i in range(len(video_names_val)):\n",
    "    if i < len(video_names_val)-1:\n",
    "        if video_names_val[i] == video_names_val[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
    "    else:\n",
    "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
    "        \n",
    "for i in range(len(frame_nums_uniq_val)):\n",
    "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
    "        \n",
    "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
    "    count_d = 0\n",
    "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
    "        if frames_num_val == frame_num_uniq_val:\n",
    "            count_d += 1\n",
    "            frames_distribution_val[frame_num_uniq_val] = count_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all the videos are of 13 frames\n",
    "\n",
    "print('Train max frames:', int(max(vids_and_frames.values())))\n",
    "print('Train min frames:', int(min(vids_and_frames.values())))\n",
    "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
    "\n",
    "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
    "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
    "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Gy0vLERf_VM"
   },
   "source": [
    "## Save frames to csv. Compress to zip archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "train_class = []\n",
    "\n",
    "for class_id in range(len(classes)):\n",
    "    images = sorted(glob.glob('data/train/' + classes[class_id] + '/*.jpg'))\n",
    "    for i in range(len(images)):\n",
    "        if ('noised' in images[i] or 'rand_contr' in images[i]\n",
    "        or 'vert_flip' in images[i] or 'hor_flip' in images[i]):\n",
    "            train_image.append(images[i].split('/')[3])\n",
    "            train_class.append(classes_dict[images[i].split('/')[3].split(']')[-1][:2]])\n",
    "        else:\n",
    "            train_image.append(images[i].split('/')[3])\n",
    "            train_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
    "        \n",
    "train_data = pd.DataFrame()\n",
    "train_data['image'] = train_image\n",
    "train_data['class'] = train_class\n",
    "\n",
    "train_data.to_csv('data/train_new.csv',header=True, index=False)\n",
    "\n",
    "\n",
    "val_image = []\n",
    "val_class = []\n",
    "\n",
    "for class_id in range(len(classes)):\n",
    "    images = sorted(glob.glob('data/validation/' + classes[class_id] + '/*.jpg'))\n",
    "    for i in range(len(images)):\n",
    "        val_image.append(images[i].split('/')[3])\n",
    "        val_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
    "val_data = pd.DataFrame()\n",
    "val_data['image'] = val_image\n",
    "val_data['class'] = val_class\n",
    "\n",
    "val_data.to_csv('data/val_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_new.csv')\n",
    "val = pd.read_csv('data/val_new.csv')\n",
    "\n",
    "y_tr = train['class']\n",
    "y_tr_dummy = pd.get_dummies(y_tr)\n",
    "\n",
    "y_val = val['class']\n",
    "y_val_dummy = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export as zip\n",
    "# from google.colab import files\n",
    "\n",
    "# !7z a -t7z train_data.zip /content/data/train -o/content\n",
    "# !7z a -t7z val_data.zip /content/data/validation -o/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files.download('val_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files.download('train_data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuHXXf4Hlo5P"
   },
   "source": [
    "## Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "UiU5TrD9T_t8",
    "outputId": "aa0354a1-e057-45a6-941a-4c1ded4794cb"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download('https://drive.google.com/uc?id=1k4_wqMaAApry4gkHHyyad1oH3ZXyDf2j&export=download', 'data.7z', quiet=True)\n",
    "gdown.download('https://drive.google.com/uc?id=1f91UumDMYm0oeVeP9xF-UijP6VrMSdc_&export=download', 'shape_predictor_68.7z', quiet=True)\n",
    "\n",
    "!7z x 'data.7z' -odata\n",
    "!7z x 'shape_predictor_68.7z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBVnWXQEIVLr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czmPPYBkIVL6"
   },
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "smjBR_mOIVMJ",
    "outputId": "32ee4567-e5c6-4f31-8836-5e931934f8a4"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.layers import *\n",
    "from keras.regularizers import *\n",
    "from keras.constraints import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "doffNfOmIVMX",
    "outputId": "c2a89ba0-5c6c-45fb-b472-776e13e19f27"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('keras_video_datagen.py')\n",
    "    os.remove('keras_video_datagen ')\n",
    "    print('removed')\n",
    "except:\n",
    "    print('file is not there')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "!wget https://gist.githubusercontent.com/swish-ds/8ea52a935fab8c0b4ff142a762995902/raw/9ad7291c8938bd876cecbe874f12c22ff06a368b/keras_video_datagen.py\n",
    "from keras_video_datagen import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BiuhhtRpIVMk",
    "outputId": "27ab53f2-230c-437c-a978-08ecf9ba3e47"
   },
   "outputs": [],
   "source": [
    "# datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "#                              vertical_flip=True, \n",
    "#                              height_shift_range=0.1, \n",
    "#                              width_shift_range=0.1, \n",
    "#                              zoom_range=0.1)\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "train_data = datagen.flow_from_directory('data/train', \n",
    "                                         target_size=(70, 140), \n",
    "                                         batch_size=5, \n",
    "                                         frames_per_step=10, \n",
    "                                         shuffle=False, \n",
    "                                         color_mode='rgb')\n",
    "\n",
    "val_data = datagen.flow_from_directory('data/validation', \n",
    "                                       target_size=(70, 140), \n",
    "                                       batch_size=5, \n",
    "                                       frames_per_step=10, \n",
    "                                       shuffle=False, \n",
    "                                       color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KnTaXhlTIVM1",
    "outputId": "887c9a73-5afc-4280-a421-081743c47dc9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_img(img, figsize=(1, 1)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid(False)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.imshow(img)\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHVyfBm0Rx4G"
   },
   "outputs": [],
   "source": [
    "x, y = train_data.next()\n",
    "\n",
    "for img in x[0]:\n",
    "    # show_img(np.squeeze(img))\n",
    "    img = Image.fromarray(img, 'RGB')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CAJGdMz3IVND",
    "outputId": "9ec7d6f9-1cf5-44ba-dcad-f577d0aae5d3"
   },
   "outputs": [],
   "source": [
    "print(math.ceil(6920/(5*10)))\n",
    "print(math.ceil((1510/(5*10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNdmLVBLnO18"
   },
   "source": [
    "## MobileNetV2 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GAdOKIiIVNY"
   },
   "outputs": [],
   "source": [
    "# video = keras.layers.Input(shape=(13, 70, 140, 3))\n",
    "\n",
    "# cnn_base = keras.applications.MobileNetV2(input_shape=(70, 140, 3), include_top=False)\n",
    "# cnn_out = keras.layers.GlobalAveragePooling2D()(cnn_base.output)\n",
    "\n",
    "# cnn = Model(input=cnn_base.input, output=cnn_out)\n",
    "# encoded_frames = keras.layers.TimeDistributed(cnn)(video)\n",
    "\n",
    "# encoded_sequence = keras.layers.LSTM(26)(encoded_frames)\n",
    "# outputs = keras.layers.Bidirectional(Dense(10, activation=\"softmax\"))(encoded_sequence)\n",
    "\n",
    "# mn_lstm = Model([video], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKo7m_uwIVNo"
   },
   "outputs": [],
   "source": [
    "# adam2 = keras.optimizers.Adam(lr=0.0001)\n",
    "# save_weights = keras.callbacks.ModelCheckpoint('mn_lstm.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "# mn_lstm.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
    "# mn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odJl8B7yIVN1"
   },
   "outputs": [],
   "source": [
    "# history = mn_lstm.fit_generator(train_data, epochs=50, steps_per_epoch=4172, \n",
    "#                               validation_data=val_data, validation_steps=127, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHDvWIR8Duok"
   },
   "source": [
    "## LipNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9KY7oBIIVOH"
   },
   "outputs": [],
   "source": [
    "model_small = keras.models.Sequential()\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 2, 2), input_shape=(10, 70, 140, 3)))\n",
    "model_small.add(Conv3D(32, (3, 5, 5), strides=(1, 2, 2), use_bias=False))\n",
    "model_small.add(BatchNormalization(momentum=0.99))\n",
    "model_small.add(Activation('relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 2, 2)))\n",
    "model_small.add(Conv3D(64, (3, 5, 5), strides=(1, 1, 1), use_bias=False))\n",
    "model_small.add(BatchNormalization(momentum=0.99))\n",
    "model_small.add(Activation('relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 1, 1)))\n",
    "model_small.add(Conv3D(96, (3, 3, 3), strides=(1, 2, 2), use_bias=False))\n",
    "model_small.add(BatchNormalization(momentum=0.99))\n",
    "model_small.add(Activation('relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "\n",
    "model_small.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_small.add(Bidirectional(GRU(50, activation='tanh')))\n",
    "\n",
    "model_small.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJuP247dIVOW"
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "sgd = optimizers.SGD(lr=1e-6, momentum=0.99, nesterov=True)\n",
    "save_weights = keras.callbacks.ModelCheckpoint('model_small.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "model_small.compile(loss='kullback_leibler_divergence', optimizer=sgd, metrics=['accuracy'])\n",
    "# model_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vhcdxPfdIVOw",
    "outputId": "e8974581-da44-409f-eb80-186dbeeb1f8a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.remove('model_small.hdf5')\n",
    "    print('file removed')\n",
    "except:\n",
    "    print('file is not there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ImcpPUrDIVPA",
    "outputId": "f1bbd371-85ec-44c1-e2be-0d41e3cfca05"
   },
   "outputs": [],
   "source": [
    "history = model_small.fit_generator(train_data, epochs=100, steps_per_epoch=139, \n",
    "                                    validation_data=val_data, validation_steps=31, callbacks=[save_weights], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnPLmidRoi3B"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('model_small.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uumHCDh8QI2B"
   },
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "YwjAhHs8QUhh",
    "outputId": "7bfcf956-58e5-444b-9fa0-4bcc18ff06ef"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,7))\n",
    "\n",
    "fig.set_facecolor(\"black\")\n",
    "fig.suptitle((\n",
    "    'SGD(lr=1e-6, momentum=0.99, nesterov=True)\\n\\\n",
    "    loss: 0.4203 - acc: 0.9942 - val_loss: 2.2343 - val_acc: 0.1987'), fontsize=16, color='white', backgroundcolor='black')\n",
    "\n",
    "ax1.plot(history.history['acc'])\n",
    "ax1.plot(history.history['val_acc'])\n",
    "ax1.set_xticks(np.arange(0, len(history.history['acc']) + 10, 10))\n",
    "ax1.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax1.tick_params(axis='both', colors='white')\n",
    "\n",
    "ax1.set_ylabel('Acuracy')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.yaxis.label.set_color('white')\n",
    "ax1.yaxis.label.set_fontsize(14)\n",
    "ax1.xaxis.label.set_color('white')\n",
    "ax1.xaxis.label.set_fontsize(14)\n",
    "\n",
    "ax1.grid(axis = 'both')\n",
    "ax1.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_xticks(np.arange(0, len(history.history['loss']) + 10, 10))\n",
    "ax2.set_yticks(np.arange(0, max(history.history['loss']) + 0.5, 0.3))\n",
    "ax2.tick_params(axis='both', colors='white')\n",
    "\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.yaxis.label.set_color('white')\n",
    "ax2.yaxis.label.set_fontsize(14)\n",
    "ax2.xaxis.label.set_color('white')\n",
    "ax2.xaxis.label.set_fontsize(14)\n",
    "\n",
    "ax2.grid(axis = 'both')\n",
    "ax2.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RHERcshmgbj-",
    "outputId": "74165e86-5d56-4d27-feb5-83defc85ecc9"
   },
   "outputs": [],
   "source": [
    "val_data2 = datagen.flow_from_directory('data/validation', \n",
    "                                       target_size=(70, 140), \n",
    "                                       batch_size=1, \n",
    "                                       frames_per_step=10, \n",
    "                                       shuffle=False, \n",
    "                                       color_mode='rgb')\n",
    "\n",
    "val_classes = []\n",
    "\n",
    "for i in list(range(0, int(len(val_data2.classes)), 10)):\n",
    "    val_classes.append(int(np.mean(np.array(val_data2.classes[i:i+10]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4dNvGkukv-8"
   },
   "outputs": [],
   "source": [
    "Y_pred = model_small.predict_generator(val_data2, 151)\n",
    "y_pred = np.rint(np.argmax(Y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "id": "5d5SfT3Z89q2",
    "outputId": "209bbc71-ad2d-4f69-ac8e-52f1ab3f756a"
   },
   "outputs": [],
   "source": [
    "target_names = 'Begin, Choose, Connection, Navigation, Next, Previous, Start, Stop, Hello, Web'\n",
    "target_names = target_names.split(', ')\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "array = confusion_matrix(val_classes, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(array, index = [i for i in target_names],\n",
    "                  columns = [i for i in target_names])\n",
    "\n",
    "ax1 = sn.heatmap(df_cm, annot=True)\n",
    "fig.set_facecolor(\"#F6F6F6\")\n",
    "fig.suptitle((\n",
    "    'SGD(lr=1e-6, momentum=0.99, nesterov=True)\\n\\\n",
    "    loss: 0.4203 - acc: 0.9942 - val_loss: 2.2343 - val_acc: 0.1987'), fontsize=16, color='Black')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "Aczn_J5lrehh",
    "outputId": "e1a652d9-718e-4388-c930-9ae962f06a37"
   },
   "outputs": [],
   "source": [
    "print(classification_report(val_classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRpnLS_y1uEr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OFUdPMpq-bWR",
    "n-1z7YV8-nh4",
    "GtTNdaei-Sua",
    "wkiSwKjkGugn",
    "ZGO1WZN4LsZ-",
    "tnS97OtiWdHM",
    "vgsim5KYZ8WA",
    "9ODHlUJzbKnc",
    "kdG0RgmccPSd",
    "5Gy0vLERf_VM",
    "yNdmLVBLnO18"
   ],
   "name": "master_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
