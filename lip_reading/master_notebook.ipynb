{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "master_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OFUdPMpq-bWR",
        "n-1z7YV8-nh4",
        "GtTNdaei-Sua",
        "wkiSwKjkGugn",
        "ZGO1WZN4LsZ-",
        "tnS97OtiWdHM",
        "vgsim5KYZ8WA",
        "9ODHlUJzbKnc",
        "kdG0RgmccPSd",
        "5Gy0vLERf_VM",
        "yNdmLVBLnO18",
        "En1DRr2UVfkO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "OFUdPMpq-bWR"
      },
      "outputs": [],
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lfytUYAj4rMB"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# files = {\n",
        "#     'F01.7z': \"https://drive.google.com/uc?id=0B4PVUqnGmrJsMXJIekMzMm1ZTzg&export=download\",\n",
        "#     \"F02.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsWnFvZkp5dm5jRTQ&export=download\",\n",
        "#     \"F04.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsZ1N3azVMa2hFdVE&export=download\",\n",
        "#     \"F05.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsRnN0ZTBGYm91dk0&export=download\",\n",
        "#     \"F06.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsTHZhS0RubnRmRXc&export=download\",\n",
        "#     \"F07.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsdzBVWUwyLU1fNFE&export=download\",\n",
        "#     \"F08.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsQVN5ZUpvbkRjRE0&export=download\",\n",
        "#     \"F09.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJscU5nT2otMjdpbTQ&export=download\",\n",
        "#     \"F10.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsM0tHdi10azVvNDQ&export=download\",\n",
        "#     \"F11.7z\": \"https://drive.google.com/u/0/uc?id=0B4PVUqnGmrJsaGtldzY4aWFJOGs&export=download\",\n",
        "#     \"M01.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsZHhNZ09UbmE2Rm8&export=download\",\n",
        "#     \"M02.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsM18tOGVEV1d3b00&export=download\",\n",
        "#     \"M04.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsdGQyaGRkb0E2UXM&export=download\",\n",
        "#     \"M07.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsc204ak0yMVBTUkU&export=download\",\n",
        "#     \"M08.7z\": \"https://drive.google.com/uc?id=0B4PVUqnGmrJsc2Fyb3BiMm9Rd2c&export=download\"\n",
        "#   }\n",
        "\n",
        "# for key in files:\n",
        "#     gdown.download(files[key], key, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "e04cIknA430K"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p data/miracl\n",
        "\n",
        "# for key in files:\n",
        "#     file = key\n",
        "#     dest = 'data/miracl/' + key.split('.')[0]\n",
        "#     !7z x $key -o$dest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Dg9ZZmtx06Ut"
      },
      "outputs": [],
      "source": [
        "gdown.download('https://drive.google.com/uc?id=1k4_wqMaAApry4gkHHyyad1oH3ZXyDf2j&export=download', 'data.7z', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=1f91UumDMYm0oeVeP9xF-UijP6VrMSdc_&export=download', 'shape_predictor_68.7z', quiet=False)\n",
        "\n",
        "!7z x 'data.7z' -odata\n",
        "!7z x 'shape_predictor_68.7z'"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "n-1z7YV8-nh4"
      },
      "outputs": [],
      "source": [
        "### Import. Setup dirs, classes and speakers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dAGNxd5E5plx"
      },
      "outputs": [],
      "source": [
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "import datetime\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import glob\n",
        "import sys\n",
        "import os\n",
        "import shutil \n",
        "import math\n",
        "import numpy as np\n",
        "from albumentations.augmentations import transforms\n",
        "import skimage\n",
        "from tqdm import tqdm\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats as s\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "colab_type": "code",
        "id": "TraZSVlJ81S6",
        "outputId": "69b32b6f-ce4e-4c5f-9432-812d0fbd55c0"
      },
      "outputs": [],
      "source": [
        "img_black = np.zeros((70, 140, 3))\n",
        "\n",
        "base_dir = 'data'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "classes = 'Begin, Choose, Connection, Navigation, Next, Previous, Start, Stop, Hello, Web'\n",
        "classes = classes.split(', ')\n",
        "\n",
        "#predictor_path = 'assests/predictors/shape_predictor_68_face_landmarks.dat'\n",
        "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "#train_people = 'F01, F02, F04, F05, F06, F07, F08, F09, M01, M02, M04'.split(', ')\n",
        "train_people = 'F01, F02, F04, F05, F06, F07, F08, F09, M01, M02, M04, F11, M08'.split(', ')\n",
        "val_people = 'F10, M07'.split(', ')\n",
        "#test_people = 'F11, M08'.split(', ')\n",
        "print(train_people)\n",
        "print(val_people)\n",
        "#print(test_people)\n",
        "\n",
        "classes_num = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
        "word_ids = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
        "classes_dict = dict(zip(classes_num, classes))\n",
        "print(classes_num)\n",
        "print(word_ids)\n",
        "print(classes_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OvFHLVr15rif"
      },
      "outputs": [],
      "source": [
        "# !rm -rf data/train\n",
        "# !rm -rf data/validation\n",
        "# !rm -rf data/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ejUpOtq79VRQ"
      },
      "outputs": [],
      "source": [
        "# for class_name in classes:\n",
        "#     train_vids_dir = os.path.join(train_dir, class_name)\n",
        "#     val_vids_dir = os.path.join(val_dir, class_name)\n",
        "#     test_vids_dir = os.path.join(test_dir, class_name)\n",
        "    \n",
        "#     os.makedirs(train_vids_dir)\n",
        "#     os.makedirs(val_vids_dir)\n",
        "#     os.makedirs(test_vids_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "GtTNdaei-Sua"
      },
      "outputs": [],
      "source": [
        "### Extract frames, crop around mouth (train and val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MIQPiolq9pDn"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for classi in tqdm(classes_num[:]):\n",
        "    for person in train_people[:]:\n",
        "        for word_id in word_ids[:]:\n",
        "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
        "                img = cv2.imread(f, 1)\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                rects = detector(gray)\n",
        "                \n",
        "                for k, rect in enumerate(rects):\n",
        "                    shape = predictor(gray, rect)\n",
        "                    \n",
        "                    x_51 = shape.part(51).x\n",
        "                    y_51 = shape.part(51).y\n",
        "                    x_57 = shape.part(57).x\n",
        "                    y_57 = shape.part(57).y\n",
        "\n",
        "                    x1_m = x_51 - 18\n",
        "                    y1_m = y_51 - 7\n",
        "                    x2_m = x_57 + 18\n",
        "                    y2_m = y_57 + 9\n",
        "\n",
        "                    offset_x_m = (70-(abs(x1_m-x2_m)))/2\n",
        "                    offset_y_m = (35-(abs(y1_m-y2_m)))/2\n",
        "\n",
        "                    img = img[int(y1_m-offset_y_m):int(y2_m+offset_y_m), int(x1_m-offset_x_m):int(x2_m+offset_x_m)]\n",
        "                    \n",
        "                    scale_percent = 200\n",
        "                    width = int(img.shape[1] * scale_percent / 100)\n",
        "                    height = int(img.shape[0] * scale_percent / 100)\n",
        "                    dim = (width, height)\n",
        "                    img = cv2.resize(img, (int(img.shape[1]*200/100), int(img.shape[0]*200/100)), interpolation=cv2.INTER_AREA) \n",
        "                    \n",
        "                counter += 1\n",
        "                \n",
        "                cv2.imwrite('data/train/' + classes_dict[classi] + '/' + classi  + '_' + person  + '_' + word_id + '_' + f[28:-4] + '.jpg', img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UI0Mk5hr_A_h"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "for classi in tqdm(classes_num[:]):\n",
        "    for person in val_people[:]:\n",
        "        for word_id in word_ids[:]:\n",
        "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
        "                img = cv2.imread(f, 1)\n",
        "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                rects = detector(gray)\n",
        "                \n",
        "                for k, rect in enumerate(rects):\n",
        "                    shape = predictor(gray, rect)\n",
        "                    \n",
        "                    x_51 = shape.part(51).x\n",
        "                    y_51 = shape.part(51).y\n",
        "                    x_57 = shape.part(57).x\n",
        "                    y_57 = shape.part(57).y\n",
        "\n",
        "                    x1_m = x_51 - 18\n",
        "                    y1_m = y_51 - 7\n",
        "                    x2_m = x_57 + 18\n",
        "                    y2_m = y_57 + 9\n",
        "\n",
        "                    offset_x_m = (70-(abs(x1_m-x2_m)))/2\n",
        "                    offset_y_m = (35-(abs(y1_m-y2_m)))/2\n",
        "\n",
        "                    img = img[int(y1_m-offset_y_m):int(y2_m+offset_y_m), int(x1_m-offset_x_m):int(x2_m+offset_x_m)]\n",
        "                    \n",
        "                    scale_percent = 200\n",
        "                    width = int(img.shape[1] * scale_percent / 100)\n",
        "                    height = int(img.shape[0] * scale_percent / 100)\n",
        "                    dim = (width, height)\n",
        "                    img = cv2.resize(img, (int(img.shape[1]*200/100), int(img.shape[0]*200/100)), interpolation=cv2.INTER_AREA) \n",
        "                    \n",
        "                counter += 1\n",
        "                \n",
        "                cv2.imwrite('data/validation/' + classes_dict[classi] + '/' + classi  + '_' + person  + '_' + word_id + '_' + f[28:-4] + '.jpg', img)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "wkiSwKjkGugn"
      },
      "outputs": [],
      "source": [
        "## Get distribution of numbers of frames across videos"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "AnopPrpIG65W"
      },
      "outputs": [],
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Jygt48ZtF7Fp"
      },
      "outputs": [],
      "source": [
        "all_images = [] # ['data/train/Begin/01_F01_01_color_001.jpg', 'data/train/Begin/01_F01_01_color_002.jpg', 'data/train/Begin/01_F01_01_color_003.jpg' ... ]             12109\n",
        "video_names = [] # ['01_F01_01', '01_F01_01', '01_F01_01' ... ]             12109\n",
        "frame_nums = [] # ['001', '002', '003' ... ]            12109\n",
        "video_names_uniq = [] # ['01_F01_01', '01_F01_02', '01_F01_03' ... ]            1100\n",
        "frame_nums_uniq = [] # ['010', '007', '010' ... ]           1100\n",
        "vids_and_frames = {} # {'01_F01_01': '010', '01_F01_02': '007', '01_F01_03': '010' ... }            1100\n",
        "frames_distribution = {} # {'005': 3, '006': 20, '007': 69 ... '022': 1}            18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NxOXrOUKGtaF"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "        all_images.append(i)\n",
        "        \n",
        "for img in all_images:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums.append(frame_num)\n",
        "    \n",
        "video_names_uniq = list(sorted(set(video_names)))\n",
        "\n",
        "for i in range(len(video_names)):\n",
        "    if i < len(video_names)-1:\n",
        "        if video_names[i] == video_names[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq.append(frame_nums[i])\n",
        "    else:\n",
        "        frame_nums_uniq.append(frame_nums[-1])\n",
        "        \n",
        "for i in range(len(video_names_uniq)):\n",
        "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
        "    \n",
        "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
        "    count_d = 0\n",
        "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
        "        if frames_num == frame_num_uniq:\n",
        "            count_d += 1\n",
        "            frames_distribution[frame_num_uniq] = count_d"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "35sUrHQjI4mX"
      },
      "outputs": [],
      "source": [
        "**Val**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DTMiUKSGHJZe"
      },
      "outputs": [],
      "source": [
        "all_images_val = [] # 1746\n",
        "video_names_val = [] # 1746\n",
        "frame_nums_val = [] # 1746\n",
        "video_names_uniq_val = [] # 200\n",
        "frame_nums_uniq_val = [] # 200\n",
        "vids_and_frames_val = {} # 200\n",
        "frames_distribution_val = {} # 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KTvNPo0XI_WX"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
        "        all_images_val.append(i)\n",
        "        \n",
        "for img in all_images_val:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names_val.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums_val.append(frame_num)\n",
        "    \n",
        "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
        "\n",
        "for i in range(len(video_names_val)):\n",
        "    if i < len(video_names_val)-1:\n",
        "        if video_names_val[i] == video_names_val[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
        "    else:\n",
        "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
        "        \n",
        "for i in range(len(frame_nums_uniq_val)):\n",
        "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
        "        \n",
        "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
        "    count_d = 0\n",
        "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
        "        if frames_num_val == frame_num_uniq_val:\n",
        "            count_d += 1\n",
        "            frames_distribution_val[frame_num_uniq_val] = count_d"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "Ip7S1zh_KsQX"
      },
      "outputs": [],
      "source": [
        "**Plot distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "colab_type": "code",
        "id": "3mDDToQXK-ah",
        "outputId": "a7eafc26-dff6-4cbc-db15-b944ea55e414"
      },
      "outputs": [],
      "source": [
        "print('Train max frames:', int(max(vids_and_frames.values())))\n",
        "print('Train min frames:', int(min(vids_and_frames.values())))\n",
        "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
        "\n",
        "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
        "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
        "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4r5HQSDVJedm"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
        "\n",
        "ax1.bar(list(frames_distribution.keys()), [int(f) for f in frames_distribution.values()], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "ax1.set_yticks(np.arange(0, max(frames_distribution.values())+2, 10))\n",
        "\n",
        "ax2.bar(list(frames_distribution_val.keys()), [int(f) for f in frames_distribution_val.values()], 0.7)\n",
        "ax2.grid(axis = 'y')\n",
        "ax2.set_yticks(np.arange(0, max(frames_distribution_val.values())+2, 10)) \n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "Ry4GW-UcBjbY"
      },
      "outputs": [],
      "source": [
        "**Stats across classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qqk7dHWHjhiH"
      },
      "outputs": [],
      "source": [
        "video_names_in_class_dict = {}\n",
        "idx = 0\n",
        "\n",
        "for classi in classes_dict.values():\n",
        "    video_names_in_class_dict[classi] = {}\n",
        "    video_names_in_class_dict[classi]['num of frames'] = 0\n",
        "    video_names_in_class_dict[classi]['num of vids'] = 0\n",
        "    video_names_in_class_dict[classi]['vids'] = []\n",
        "    video_names_in_class_dict[classi]['frames per vid'] = []\n",
        "    video_names_in_class_dict[classi]['mean frames per vid'] = 0\n",
        "\n",
        "    all_images_in_class = []\n",
        "    video_names = []\n",
        "    video_names_uniq_in_class = []\n",
        "    frame_nums_in_class = []\n",
        "    frame_nums_uniq_in_class = []\n",
        "    vids_and_frames_in_class = {}\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "        all_images_in_class.append(i)\n",
        "        counter += 1\n",
        "\n",
        "    video_names_in_class_dict[classi]['num of frames'] = counter\n",
        "\n",
        "    for img in all_images_in_class:\n",
        "        img_name = img.split('/')[-1].split('.')[0]\n",
        "        video_name = img_name[:9]\n",
        "        video_names.append(video_name)\n",
        "        frame_num = img_name[-3:]\n",
        "        frame_nums_in_class.append(frame_num)\n",
        "\n",
        "    video_names_uniq_in_class = list(sorted(set(video_names)))\n",
        "\n",
        "    video_names_in_class_dict[classi]['vids'] = video_names_uniq_in_class\n",
        "    video_names_in_class_dict[classi]['num of vids'] = len(video_names_uniq_in_class)\n",
        "\n",
        "    for i in range(len(video_names)):\n",
        "        if i < len(video_names)-1:\n",
        "            if video_names[i] == video_names[i+1]:\n",
        "                pass\n",
        "            else:\n",
        "                frame_nums_uniq_in_class.append(frame_nums_in_class[i])\n",
        "        else:\n",
        "            frame_nums_uniq_in_class.append(frame_nums_in_class[-1])\n",
        "\n",
        "    for i in range(len(video_names_uniq_in_class)):\n",
        "        vids_and_frames_in_class[video_names_uniq_in_class[i]] = frame_nums_uniq_in_class[i]\n",
        "\n",
        "    video_names_in_class_dict[classi]['frames per vid'] = list(vids_and_frames_in_class.values())\n",
        "    video_names_in_class_dict[classi]['mean frames per vid'] = np.array(list(list(video_names_in_class_dict.values())[idx].values())[3]).astype(np.float).mean()\n",
        "\n",
        "    idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "XgN-uVzgCL-y",
        "outputId": "2d1649aa-6ceb-4d4b-f30e-c3eacdd47f34"
      },
      "outputs": [],
      "source": [
        "[list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "colab_type": "code",
        "id": "LArg_8oG5zpA",
        "outputId": "6c347fac-96be-4862-b2a4-21698c49b02a"
      },
      "outputs": [],
      "source": [
        "# График слева - сколько всего фреймов в видосах у каждого класса + количество видосов. Изначально их по 130 у каждого класса\n",
        "# График справа - среднее число кадров в видео у каждого класса\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
        "\n",
        "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "ax1.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))]) + 2, 150))\n",
        "\n",
        "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "#ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]) + 2, 150))\n",
        "\n",
        "ax2.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))], 0.7)\n",
        "ax2.grid(axis = 'y')\n",
        "ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))]) + 2, 1))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9B4mWFtw5vdq"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# import google\n",
        "\n",
        "# with open('video_names_in_class.json', 'w') as jfile:\n",
        "#     json.dump(video_names_in_class, jfile)\n",
        "\n",
        "# google.colab.files.download('video_names_in_class.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "ZGO1WZN4LsZ-"
      },
      "outputs": [],
      "source": [
        "## Delete videos either too short or too long"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "ubYd-5qXL3EN"
      },
      "outputs": [],
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5hQ-K0fLKkVF"
      },
      "outputs": [],
      "source": [
        "comp = {}\n",
        "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
        "    comp[frame_num_uniq] = {}\n",
        "    comp[frame_num_uniq]['class'] = []\n",
        "    comp[frame_num_uniq]['person'] = []\n",
        "    comp[frame_num_uniq]['video_names'] = []\n",
        "    comp[frame_num_uniq]['videos_count'] = 0\n",
        "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
        "        if frames_num == frame_num_uniq:\n",
        "            if vid_name.split('_')[0] not in comp[frame_num_uniq]['class']:\n",
        "                comp[frame_num_uniq]['class'].append(vid_name.split('_')[0])\n",
        "            if vid_name.split('_')[1] not in comp[frame_num_uniq]['person']:\n",
        "                comp[frame_num_uniq]['person'].append(vid_name.split('_')[1])\n",
        "            if vid_name not in comp[frame_num_uniq]['video_names']:\n",
        "                comp[frame_num_uniq]['video_names'].append(vid_name)\n",
        "                comp[frame_num_uniq]['videos_count'] += 1\n",
        "\n",
        "# get the most common number of frames\n",
        "maxer = 0\n",
        "maxer_key = 0\n",
        "for key in frames_distribution:\n",
        "    if frames_distribution[key] >= maxer:\n",
        "        maxer = frames_distribution[key]\n",
        "        maxer_key = key\n",
        "\n",
        "# videos either too short or too long (with num. of frames not in [7, 8, 9, 10, 11, 12, 13])\n",
        "# total to 233 vids\n",
        "abnormal_vids = []\n",
        "for key in comp:\n",
        "    for video_name in comp[key]['video_names']:\n",
        "        if int(key) not in range(int(maxer_key)-3, int(maxer_key)+1):\n",
        "            abnormal_vids.append(video_name)\n",
        "abnormal_vids = sorted(abnormal_vids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ByaAhsE2udno"
      },
      "outputs": [],
      "source": [
        "list(range(int(maxer_key)-3, int(maxer_key)+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1XrXXmkLMA_G"
      },
      "outputs": [],
      "source": [
        "# remove not needed\n",
        "# debug note: the last print must return \"Removed 233 videos\" - the number of videos to remove\n",
        "\n",
        "counter = 0\n",
        "added = 0\n",
        "matched = []\n",
        "for classi in list(classes_dict.values()):\n",
        "    removed = 0\n",
        "    for abnormal_vid in abnormal_vids:\n",
        "        for vid in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "            if abnormal_vid == vid.split('/')[-1].split('.')[0].split('_color')[0]:\n",
        "                os.remove(vid)\n",
        "                #!rm $vid\n",
        "                matched.append(abnormal_vid)\n",
        "                counter += 1\n",
        "                removed += 1\n",
        "    print(classi, 'Removed', counter, '(' , removed, 'in this dir)')\n",
        "\n",
        "print()\n",
        "print('Removed', len(set(matched)), 'videos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Hk13BEvCMqet"
      },
      "outputs": [],
      "source": [
        "# optionally, save the distribution as a json and visualize\n",
        "# https://codebeautify.org/jsonviewer\n",
        "\n",
        "# import json\n",
        "# from google.colab import files\n",
        "\n",
        "# with open('comp.json', 'w') as jfile:\n",
        "#     json.dump(comp, jfile)\n",
        "\n",
        "# files.download('comp.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vfdaDfNgNT-L"
      },
      "outputs": [],
      "source": [
        "comp_val = {}\n",
        "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
        "    comp_val[frame_num_uniq_val] = {}\n",
        "    comp_val[frame_num_uniq_val]['class'] = []\n",
        "    comp_val[frame_num_uniq_val]['person'] = []\n",
        "    comp_val[frame_num_uniq_val]['video_names'] = []\n",
        "    comp_val[frame_num_uniq_val]['videos_count'] = 0\n",
        "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
        "        if frames_num_val == frame_num_uniq_val:\n",
        "            if vid_name_val.split('_')[0] not in comp_val[frame_num_uniq_val]['class']:\n",
        "                comp_val[frame_num_uniq_val]['class'].append(vid_name_val.split('_')[0])\n",
        "            if vid_name_val.split('_')[1] not in comp_val[frame_num_uniq_val]['person']:\n",
        "                comp_val[frame_num_uniq_val]['person'].append(vid_name_val.split('_')[1])\n",
        "            if vid_name_val not in comp_val[frame_num_uniq_val]['video_names']:\n",
        "                comp_val[frame_num_uniq_val]['video_names'].append(vid_name_val)\n",
        "                comp_val[frame_num_uniq_val]['videos_count'] += 1\n",
        "\n",
        "maxer = 0\n",
        "maxer_key = 0\n",
        "for key in frames_distribution_val:\n",
        "    if frames_distribution_val[key] >= maxer:\n",
        "        maxer = frames_distribution_val[key]\n",
        "        maxer_key = key\n",
        "\n",
        "# videos either too short or too long (with num. of frames not in [6, 7, 8, 9, 10])\n",
        "# total to 35 vids\n",
        "abnormal_vids = []\n",
        "for key in comp_val:\n",
        "    for video_name in comp_val[key]['video_names']:\n",
        "        if int(key) not in range(int(maxer_key)-1, int(maxer_key)+3):\n",
        "            abnormal_vids.append(video_name)\n",
        "abnormal_vids = sorted(abnormal_vids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JRSXG1gbudpS"
      },
      "outputs": [],
      "source": [
        "list(range(int(maxer_key)-1, int(maxer_key)+3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U6dOo4e4PP1g"
      },
      "outputs": [],
      "source": [
        "# debug note: the past print must return \"Removed 35 videos\" - the number of videos to remove\n",
        "counter = 0\n",
        "added = 0\n",
        "matched = []\n",
        "for classi in list(classes_dict.values()):\n",
        "    removed = 0\n",
        "    for abnormal_vid in abnormal_vids:\n",
        "        for vid in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
        "            if abnormal_vid == vid.split('/')[-1].split('.')[0].split('_color')[0]:\n",
        "                os.remove(vid)\n",
        "                matched.append(abnormal_vid)\n",
        "                counter += 1\n",
        "                removed += 1\n",
        "    print(classi, 'Removed', counter, '(' , removed, 'in this dir)')\n",
        "\n",
        "print()\n",
        "print('Removed', len(set(matched)), 'videos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z-Cwx0A6P5Cc"
      },
      "outputs": [],
      "source": [
        "# optionally, save the distribution as a json and visualize\n",
        "# https://codebeautify.org/jsonviewer\n",
        "\n",
        "# import json\n",
        "# from google.colab import files\n",
        "\n",
        "# with open('comp_val.json', 'w') as jfile:\n",
        "#     json.dump(comp_val, jfile)\n",
        "\n",
        "# files.download('comp_val.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "tnS97OtiWdHM"
      },
      "outputs": [],
      "source": [
        "## Check the distribution again"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "-IoNyVjBWdHR"
      },
      "outputs": [],
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BN2g4vnJWdHS"
      },
      "outputs": [],
      "source": [
        "all_images = [] # 8699 vs 12109\n",
        "video_names = [] # 8699 vs 12109\n",
        "frame_nums = [] # 8699 vs 12109\n",
        "video_names_uniq = [] # 867 vs 1100\n",
        "frame_nums_uniq = [] # 867 vs 1100\n",
        "vids_and_frames = {} # 867 vs 1100\n",
        "frames_distribution = {} # 7 vs 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tJ2ouDyDWdHX"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "        all_images.append(i)\n",
        "        \n",
        "for img in all_images:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums.append(frame_num)\n",
        "    \n",
        "video_names_uniq = list(sorted(set(video_names)))\n",
        "\n",
        "for i in range(len(video_names)):\n",
        "    if i < len(video_names)-1:\n",
        "        if video_names[i] == video_names[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq.append(frame_nums[i])\n",
        "    else:\n",
        "        frame_nums_uniq.append(frame_nums[-1])\n",
        "        \n",
        "for i in range(len(video_names_uniq)):\n",
        "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
        "    \n",
        "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
        "    count_d = 0\n",
        "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
        "        if frames_num == frame_num_uniq:\n",
        "            count_d += 1\n",
        "            frames_distribution[frame_num_uniq] = count_d"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "JVUWOj6-WdHa"
      },
      "outputs": [],
      "source": [
        "**Val**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0WqceAcTWdHa"
      },
      "outputs": [],
      "source": [
        "all_images_val = [] # 1364 vs 1746\n",
        "video_names_val = [] # 1364 vs 1746\n",
        "frame_nums_val = [] # 1364 vs 1746\n",
        "video_names_uniq_val = [] # 165 vs 200\n",
        "frame_nums_uniq_val = [] # 165 vs 200\n",
        "vids_and_frames_val = {} # 165 vs 200\n",
        "frames_distribution_val = {} # 5 vs 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sB9F4z2AWdHf"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
        "        all_images_val.append(i)\n",
        "        \n",
        "for img in all_images_val:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names_val.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums_val.append(frame_num)\n",
        "    \n",
        "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
        "\n",
        "for i in range(len(video_names_val)):\n",
        "    if i < len(video_names_val)-1:\n",
        "        if video_names_val[i] == video_names_val[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
        "    else:\n",
        "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
        "        \n",
        "for i in range(len(frame_nums_uniq_val)):\n",
        "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
        "        \n",
        "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
        "    count_d = 0\n",
        "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
        "        if frames_num_val == frame_num_uniq_val:\n",
        "            count_d += 1\n",
        "            frames_distribution_val[frame_num_uniq_val] = count_d"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "DiUpwWRmWdHj"
      },
      "outputs": [],
      "source": [
        "**Plot distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d0GEfQKYWdHj"
      },
      "outputs": [],
      "source": [
        "print('Train max frames:', int(max(vids_and_frames.values())))\n",
        "print('Train min frames:', int(min(vids_and_frames.values())))\n",
        "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
        "\n",
        "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
        "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
        "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rjiqf7waWdHm"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
        "\n",
        "ax1.bar(list(frames_distribution.keys()), [int(f) for f in frames_distribution.values()], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "ax1.set_yticks(np.arange(0, max(frames_distribution.values())+2, 10))\n",
        "\n",
        "ax2.bar(list(frames_distribution_val.keys()), [int(f) for f in frames_distribution_val.values()], 0.7)\n",
        "ax2.grid(axis = 'y')\n",
        "ax2.set_yticks(np.arange(0, max(frames_distribution_val.values())+2, 10)) \n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "Qsj6Mh0KB0UV"
      },
      "outputs": [],
      "source": [
        "**Stats across classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MK0BIjdvBzrH"
      },
      "outputs": [],
      "source": [
        "video_names_in_class_dict = {}\n",
        "idx = 0\n",
        "\n",
        "for classi in classes_dict.values():\n",
        "    video_names_in_class_dict[classi] = {}\n",
        "    video_names_in_class_dict[classi]['num of frames'] = 0\n",
        "    video_names_in_class_dict[classi]['num of vids'] = 0\n",
        "    video_names_in_class_dict[classi]['vids'] = []\n",
        "    video_names_in_class_dict[classi]['frames per vid'] = []\n",
        "    video_names_in_class_dict[classi]['mean frames per vid'] = 0\n",
        "\n",
        "    all_images_in_class = []\n",
        "    video_names = []\n",
        "    video_names_uniq_in_class = []\n",
        "    frame_nums_in_class = []\n",
        "    frame_nums_uniq_in_class = []\n",
        "    vids_and_frames_in_class = {}\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "        all_images_in_class.append(i)\n",
        "        counter += 1\n",
        "\n",
        "    video_names_in_class_dict[classi]['num of frames'] = counter\n",
        "\n",
        "    for img in all_images_in_class:\n",
        "        img_name = img.split('/')[-1].split('.')[0]\n",
        "        video_name = img_name[:9]\n",
        "        video_names.append(video_name)\n",
        "        frame_num = img_name[-3:]\n",
        "        frame_nums_in_class.append(frame_num)\n",
        "\n",
        "    video_names_uniq_in_class = list(sorted(set(video_names)))\n",
        "\n",
        "    video_names_in_class_dict[classi]['vids'] = video_names_uniq_in_class\n",
        "    video_names_in_class_dict[classi]['num of vids'] = len(video_names_uniq_in_class)\n",
        "\n",
        "    for i in range(len(video_names)):\n",
        "        if i < len(video_names)-1:\n",
        "            if video_names[i] == video_names[i+1]:\n",
        "                pass\n",
        "            else:\n",
        "                frame_nums_uniq_in_class.append(frame_nums_in_class[i])\n",
        "        else:\n",
        "            frame_nums_uniq_in_class.append(frame_nums_in_class[-1])\n",
        "\n",
        "    for i in range(len(video_names_uniq_in_class)):\n",
        "        vids_and_frames_in_class[video_names_uniq_in_class[i]] = frame_nums_uniq_in_class[i]\n",
        "\n",
        "    video_names_in_class_dict[classi]['frames per vid'] = list(vids_and_frames_in_class.values())\n",
        "    video_names_in_class_dict[classi]['mean frames per vid'] = np.array(list(list(video_names_in_class_dict.values())[idx].values())[3]).astype(np.float).mean()\n",
        "\n",
        "    idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XTTp7YeTCv-w"
      },
      "outputs": [],
      "source": [
        "[list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ejoUsxonB4ve"
      },
      "outputs": [],
      "source": [
        "# Видим просадку у класса Navigation - было 130 видео, стало 80. Число кадров соотевтстенно тоже упало.\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,8))\n",
        "\n",
        "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "ax1.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[0] for f in range(len(classes))]) + 2, 150))\n",
        "\n",
        "ax1.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))], 0.7)\n",
        "ax1.grid(axis = 'y')\n",
        "#ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[1] for f in range(len(classes))]) + 2, 150))\n",
        "\n",
        "ax2.bar(list(video_names_in_class_dict.keys()), [list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))], 0.7)\n",
        "ax2.grid(axis = 'y')\n",
        "ax2.set_yticks(np.arange(0, max([list(list(video_names_in_class_dict.values())[f].values())[4] for f in range(len(classes))]) + 2, 1))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "vgsim5KYZ8WA"
      },
      "outputs": [],
      "source": [
        "## Perform padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MC_7o5NdQHgw"
      },
      "outputs": [],
      "source": [
        "# make sure that padding's shape equals to (w x h x c) of the train images\n",
        "\n",
        "print(img_black.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DJh-wMZXaqEl"
      },
      "outputs": [],
      "source": [
        "target_frame_num = int(max(vids_and_frames.values())) #13\n",
        "target_frame_num_val = int(max(vids_and_frames_val.values())) # 10\n",
        "print(target_frame_num)\n",
        "print(target_frame_num_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Qr6tzj3uWpzv"
      },
      "outputs": [],
      "source": [
        "# pad train\n",
        "\n",
        "for classi in classes_dict.keys():\n",
        "    for vid in vids_and_frames.keys():\n",
        "        if classi == vid.split('_')[0]:\n",
        "            if int(vids_and_frames[vid]) < target_frame_num:\n",
        "                for i in range(1, (target_frame_num - int(vids_and_frames[vid]) + 1)):\n",
        "                    cv2.imwrite('data/train/' + classes_dict[classi] + '/' + vid + '_' + 'color' + '_' + str(int(vids_and_frames[vid]) + i).zfill(3) + '.jpg', img_black)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_Y9Lq2IKaknl"
      },
      "outputs": [],
      "source": [
        "# pad val\n",
        "# has to be padded to 13 ... not 10\n",
        "\n",
        "for classi in classes_dict.keys():\n",
        "    for vid in vids_and_frames_val.keys():\n",
        "        if classi == vid.split('_')[0]:\n",
        "            if int(vids_and_frames_val[vid]) < target_frame_num:\n",
        "                for i in range(1, (target_frame_num - int(vids_and_frames_val[vid]) + 1)):\n",
        "                    cv2.imwrite('data/validation/' + classes_dict[classi] + '/' + vid + '_' + 'color' + '_' + str(int(vids_and_frames_val[vid]) + i).zfill(3) + '.jpg', img_black)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "9ODHlUJzbKnc"
      },
      "outputs": [],
      "source": [
        "## Check the distribution the last time to be sure"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "xtWQZGFYbKnf"
      },
      "outputs": [],
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WVjEL7EfbKng"
      },
      "outputs": [],
      "source": [
        "all_images = [] # 11271 vs 8699 vs 12109\n",
        "video_names = [] # 11271 vs 8699 vs 12109\n",
        "frame_nums = [] # 11271 vs 8699 vs 12109\n",
        "video_names_uniq = [] # 867 vs 867 vs 1100\n",
        "frame_nums_uniq = [] # 867 vs 867 vs 1100\n",
        "vids_and_frames = {} # 867 vs 867 vs 1100\n",
        "frames_distribution = {} # 1 vs 7 vs 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M9EcokmibKnk"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
        "        all_images.append(i)\n",
        "        \n",
        "for img in all_images:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums.append(frame_num)\n",
        "    \n",
        "video_names_uniq = list(sorted(set(video_names)))\n",
        "\n",
        "for i in range(len(video_names)):\n",
        "    if i < len(video_names)-1:\n",
        "        if video_names[i] == video_names[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq.append(frame_nums[i])\n",
        "    else:\n",
        "        frame_nums_uniq.append(frame_nums[-1])\n",
        "        \n",
        "for i in range(len(video_names_uniq)):\n",
        "    vids_and_frames[video_names_uniq[i]] = frame_nums_uniq[i]\n",
        "    \n",
        "for frame_num_uniq in sorted(set(frame_nums_uniq)):\n",
        "    count_d = 0\n",
        "    for vid_name, frames_num in list(vids_and_frames.items()):\n",
        "        if frames_num == frame_num_uniq:\n",
        "            count_d += 1\n",
        "            frames_distribution[frame_num_uniq] = count_d"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "CMhtEhT_bKns"
      },
      "outputs": [],
      "source": [
        "**Val**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HogYINDIbKnu"
      },
      "outputs": [],
      "source": [
        "all_images_val = [] # 1650 vs 1364 vs 1746\n",
        "video_names_val = [] # 1650 vs 1364 vs 1746\n",
        "frame_nums_val = [] # 1650 vs 1364 vs 1746\n",
        "video_names_uniq_val = [] # 165 vs 165 vs 200\n",
        "frame_nums_uniq_val = [] # 165 vs 165 vs 200\n",
        "vids_and_frames_val = {} # 165 vs 165 vs 200\n",
        "frames_distribution_val = {} # 1 vs 5 vs 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sMprHDM3bKny"
      },
      "outputs": [],
      "source": [
        "for classi in classes_dict.values():\n",
        "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
        "        all_images_val.append(i)\n",
        "        \n",
        "for img in all_images_val:\n",
        "    img_name = img.split('/')[-1].split('.')[0]\n",
        "    video_name = img_name[:9]\n",
        "    video_names_val.append(video_name)\n",
        "    frame_num = img_name[-3:]\n",
        "    frame_nums_val.append(frame_num)\n",
        "    \n",
        "video_names_uniq_val = list(sorted(set(video_names_val)))\n",
        "\n",
        "for i in range(len(video_names_val)):\n",
        "    if i < len(video_names_val)-1:\n",
        "        if video_names_val[i] == video_names_val[i+1]:\n",
        "            pass\n",
        "        else:\n",
        "            frame_nums_uniq_val.append(frame_nums_val[i])\n",
        "    else:\n",
        "        frame_nums_uniq_val.append(frame_nums_val[-1])\n",
        "        \n",
        "for i in range(len(frame_nums_uniq_val)):\n",
        "        vids_and_frames_val[video_names_uniq_val[i]] = frame_nums_uniq_val[i]\n",
        "        \n",
        "for frame_num_uniq_val in sorted(set(frame_nums_uniq_val)):\n",
        "    count_d = 0\n",
        "    for vid_name_val, frames_num_val in list(vids_and_frames_val.items()):\n",
        "        if frames_num_val == frame_num_uniq_val:\n",
        "            count_d += 1\n",
        "            frames_distribution_val[frame_num_uniq_val] = count_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mrd5buwybKn3"
      },
      "outputs": [],
      "source": [
        "# Now all the videos are of 13 frames\n",
        "\n",
        "print('Train max frames:', int(max(vids_and_frames.values())))\n",
        "print('Train min frames:', int(min(vids_and_frames.values())))\n",
        "print('Train mean frames:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()), end = '\\n\\n')\n",
        "\n",
        "print('Val max frames:', int(max(vids_and_frames_val.values())))\n",
        "print('Val min frames:', int(min(vids_and_frames_val.values())))\n",
        "print('Val mean frames:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "kdG0RgmccPSd"
      },
      "outputs": [],
      "source": [
        "## Perform augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c7aq-WY1bFbV"
      },
      "outputs": [],
      "source": [
        "# Вспомогательная функция для удобного переименования видео.\n",
        "\n",
        "# def get_idx(classi):\n",
        "#     start_idx = (len('data/train/') + len(classi)+1)\n",
        "#     return start_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dzOnJELIbFdW"
      },
      "outputs": [],
      "source": [
        "# # Оставил только горизонтальный поворот и шум.\n",
        "\n",
        "# for classi in classes:\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     files_clean = [file_clean for file_clean in sorted(files)\n",
        "#                 if 'vert_flip' not in file_clean\n",
        "#                 and 'hor_flip' not in file_clean\n",
        "#                 and 'rot_45' not in file_clean\n",
        "#                 and 'rot_315' not in file_clean\n",
        "#                 and 'rot_135' not in file_clean\n",
        "#                 and 'rot_225' not in file_clean\n",
        "#                 and 'rot_90' not in file_clean\n",
        "#                 and 'rot_270' not in file_clean\n",
        "#                 and 'rand_contr' not in file_clean\n",
        "#                 and 'noised' not in file_clean]\n",
        "#     files_rem = [file_rem for file_rem in sorted(files)\n",
        "#                 if 'vert_flip' in file_rem\n",
        "#                 or 'hor_flip' in file_rem\n",
        "#                 or 'rot_45' in file_rem\n",
        "#                 or 'rot_315' in file_rem\n",
        "#                 or 'rot_135' in file_rem\n",
        "#                 or 'rot_225' in file_rem\n",
        "#                 or 'rot_90' in file_rem\n",
        "#                 or 'rot_270' in file_rem\n",
        "#                 or 'rand_contr' in file_rem\n",
        "#                 or 'noised' in file_rem]\n",
        "#     for file_to_rem in files_rem: os.remove(file_to_rem)\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     print(classi, 'dir: removed', len(files_rem), 'files.', len(files), 'to be augmented')\n",
        "\n",
        "#     for file_to_hor_flip in files_clean:\n",
        "#         img = cv2.imread(file_to_hor_flip, 1)\n",
        "#         img = transforms.HorizontalFlip().apply(img)\n",
        "#         file_to_write = file_to_hor_flip[:get_idx(classi)] + '[hor_flip]' + file_to_hor_flip[get_idx(classi):]\n",
        "#         cv2.imwrite(file_to_write, img)\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     files_hor_flipped = [file_hor_flipped for file_hor_flipped in sorted(files) if 'hor_flip' in file_hor_flipped]\n",
        "#     print('Horizontally flipped', len(files_hor_flipped), 'files.', len(files), 'in the directory')\n",
        "    \n",
        "#     for file_to_vert_flip in files_clean:\n",
        "#         img = cv2.imread(file_to_vert_flip, 1)\n",
        "#         img = transforms.VerticalFlip().apply(img)\n",
        "#         file_to_write = file_to_vert_flip[:get_idx(classi)] + '[vert_flip]' + file_to_vert_flip[get_idx(classi):]\n",
        "#         cv2.imwrite(file_to_write, img)\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     files_vert_flipped = [file_vert_flipped for file_vert_flipped in sorted(files) if 'vert_flip' in file_vert_flipped]\n",
        "#     print('Vertically flipped', len(files_vert_flipped), 'files.', len(files), 'in the directory')\n",
        "    \n",
        "#     files_not_rand_contred = [file_not_rand_contred for file_not_rand_contred in sorted(files) if 'rand_contr' not in file_not_rand_contred]\n",
        "#     for file_to_noise in files_not_rand_contred:\n",
        "#         if '[hor_flip]' in file_to_noise:\n",
        "#             img = cv2.imread(file_to_noise, 1)\n",
        "#             gauss = np.random.uniform(0, 64, img.size)\n",
        "#             gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
        "#             img = cv2.add(img,gauss)\n",
        "#             file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#         elif '[vert_flip]' in file_to_noise:\n",
        "#             img = cv2.imread(file_to_noise, 1)\n",
        "#             gauss = np.random.uniform(0, 64, img.size)\n",
        "#             gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
        "#             img = cv2.add(img,gauss)\n",
        "#             file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#         else:\n",
        "#             img = cv2.imread(file_to_noise, 1)\n",
        "#             gauss = np.random.uniform(0, 64, img.size)\n",
        "#             gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
        "#             img = cv2.add(img,gauss)\n",
        "#             file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     files_noised = [file_noised for file_noised in sorted(files) if 'noised' in file_noised]\n",
        "#     print('Noised', len(files_noised), 'files.', len(files), 'in the directory')\n",
        "\n",
        "#     files_not_noised = [file_not_noised for file_not_noised in sorted(files) if 'noised' not in file_not_noised ]    \n",
        "#     for file_to_rand_contr in files_not_noised:\n",
        "#         if '[hor_flip]' in file_to_rand_contr:\n",
        "#             img = cv2.imread(file_to_rand_contr, 1)\n",
        "#             img = transforms.RandomContrast().apply(img)\n",
        "#             file_to_write = file_to_rand_contr[:get_idx(classi)] + '[rand_contr]' + file_to_rand_contr[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#         elif '[vert_flip]' in file_to_rand_contr:\n",
        "#             img = cv2.imread(file_to_rand_contr, 1)\n",
        "#             img = transforms.RandomContrast().apply(img)\n",
        "#             file_to_write = file_to_rand_contr[:get_idx(classi)] + '[rand_contr]' + file_to_rand_contr[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#         else:\n",
        "#             img = cv2.imread(file_to_rand_contr, 1)\n",
        "#             img = transforms.RandomContrast().apply(img)\n",
        "#             file_to_write = file_to_rand_contr[:get_idx(classi)] + '[rand_contr]' + file_to_rand_contr[get_idx(classi):]\n",
        "#             cv2.imwrite(file_to_write, img)\n",
        "#     files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
        "#     files_rand_contred = [file_rand_contred for file_rand_contred in sorted(files) if 'rand_contr' in file_rand_contred]\n",
        "#     print('Random contrasted', len(files_rand_contred), 'files.', len(files), 'in the directory')\n",
        "    \n",
        "    \n",
        "#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "5Gy0vLERf_VM"
      },
      "outputs": [],
      "source": [
        "## Save frames to csv. Compress to zip archives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-PsQT7GDfjnv"
      },
      "outputs": [],
      "source": [
        "train_image = []\n",
        "train_class = []\n",
        "\n",
        "for class_id in range(len(classes)):\n",
        "    images = sorted(glob.glob('data/train/' + classes[class_id] + '/*.jpg'))\n",
        "    for i in range(len(images)):\n",
        "        if ('noised' in images[i] or 'rand_contr' in images[i]\n",
        "        or 'vert_flip' in images[i] or 'hor_flip' in images[i]):\n",
        "            train_image.append(images[i].split('/')[3])\n",
        "            train_class.append(classes_dict[images[i].split('/')[3].split(']')[-1][:2]])\n",
        "        else:\n",
        "            train_image.append(images[i].split('/')[3])\n",
        "            train_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
        "        \n",
        "train_data = pd.DataFrame()\n",
        "train_data['image'] = train_image\n",
        "train_data['class'] = train_class\n",
        "\n",
        "train_data.to_csv('data/train_new.csv',header=True, index=False)\n",
        "\n",
        "\n",
        "val_image = []\n",
        "val_class = []\n",
        "\n",
        "for class_id in range(len(classes)):\n",
        "    images = sorted(glob.glob('data/validation/' + classes[class_id] + '/*.jpg'))\n",
        "    for i in range(len(images)):\n",
        "        val_image.append(images[i].split('/')[3])\n",
        "        val_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
        "val_data = pd.DataFrame()\n",
        "val_data['image'] = val_image\n",
        "val_data['class'] = val_class\n",
        "\n",
        "val_data.to_csv('data/val_new.csv',header=True, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yt1Sv8UFgZ5l"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('data/train_new.csv')\n",
        "val = pd.read_csv('data/val_new.csv')\n",
        "\n",
        "y_tr = train['class']\n",
        "y_tr_dummy = pd.get_dummies(y_tr)\n",
        "\n",
        "y_val = val['class']\n",
        "y_val_dummy = pd.get_dummies(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "yBGHpqrMf-yM",
        "outputId": "b9e0d8f6-cc3e-4319-ab43-9376db858b76"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "YOO6O69Mf-0g",
        "outputId": "10570db3-44a5-4f17-c7b6-4cfdb6588d25"
      },
      "outputs": [],
      "source": [
        "train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "14tT_8EQgmkM",
        "outputId": "d0d44166-5031-4932-d65c-7f438673d11f"
      },
      "outputs": [],
      "source": [
        "val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "colab_type": "code",
        "id": "c793eGsQgoA6",
        "outputId": "8bc4d404-1f54-431f-b852-2473194720fe"
      },
      "outputs": [],
      "source": [
        "val.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "WAlrB2bOgpPE",
        "outputId": "474dcfbf-e7e0-4c3f-c18b-7be9084ff380"
      },
      "outputs": [],
      "source": [
        "# 101439 train images\n",
        "# 1650 val images\n",
        "\n",
        "print(len(train))\n",
        "print(len(val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c5xV9M-khKVV"
      },
      "outputs": [],
      "source": [
        "# # export as zip\n",
        "# from google.colab import files\n",
        "\n",
        "# !7z a -t7z train_data.zip /content/data/train -o/content\n",
        "# !7z a -t7z val_data.zip /content/data/validation -o/content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ectu7YRvi2Yh"
      },
      "outputs": [],
      "source": [
        "# files.download('val_data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ce4bEoI_i2Wc"
      },
      "outputs": [],
      "source": [
        "# files.download('train_data.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "PuHXXf4Hlo5P"
      },
      "outputs": [],
      "source": [
        "## Prepare data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.InteractiveSession(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import image, sequence\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.layers import *\n",
        "from keras.regularizers import *\n",
        "from keras.constraints import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "#!wget https://gist.githubusercontent.com/swish-ds/8ea52a935fab8c0b4ff142a762995902/raw/9ea4abd55b988860751873f3cdf6b93897010364/keras_video_datagen.py\n",
        "from video_datagen import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(horizontal_flip=True, height_shift_range=0.2, width_shift_range=0.2, zoom_range=0.2)\n",
        "train_data = datagen.flow_from_directory('data/train', \n",
        "                                         target_size=(70, 140), \n",
        "                                         batch_size=5, \n",
        "                                         frames_per_step=10, \n",
        "                                         shuffle=False)\n",
        "val_data = datagen.flow_from_directory('data/validation', \n",
        "                                       target_size=(70, 140), \n",
        "                                       batch_size=5, \n",
        "                                       frames_per_step=10, \n",
        "                                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_img(img, figsize=(8, 8)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.grid(False)\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.imshow(img)\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(math.ceil(6920/(5*10)))\n",
        "print(math.ceil((1510/(5*10))))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "yNdmLVBLnO18"
      },
      "outputs": [],
      "source": [
        "## MobileNetV2 + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# video = keras.layers.Input(shape=(13, 70, 140, 3))\n",
        "\n",
        "# cnn_base = keras.applications.MobileNetV2(input_shape=(70, 140, 3), include_top=False)\n",
        "# cnn_out = keras.layers.GlobalAveragePooling2D()(cnn_base.output)\n",
        "\n",
        "# cnn = Model(input=cnn_base.input, output=cnn_out)\n",
        "# encoded_frames = keras.layers.TimeDistributed(cnn)(video)\n",
        "\n",
        "# encoded_sequence = keras.layers.LSTM(26)(encoded_frames)\n",
        "# outputs = keras.layers.Bidirectional(Dense(10, activation=\"softmax\"))(encoded_sequence)\n",
        "\n",
        "# mn_lstm = Model([video], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adam2 = keras.optimizers.Adam(lr=0.0001)\n",
        "# save_weights = keras.callbacks.ModelCheckpoint('mn_lstm.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "# mn_lstm.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
        "# mn_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# history = mn_lstm.fit_generator(train_data, epochs=50, steps_per_epoch=4172, \n",
        "#                               validation_data=val_data, validation_steps=127, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "QHDvWIR8Duok"
      },
      "outputs": [],
      "source": [
        "## LipNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_small = keras.models.Sequential()\n",
        "\n",
        "model_small.add(ZeroPadding3D(padding=(1, 2, 2), input_shape=(10, 70, 140, 3)))\n",
        "model_small.add(Conv3D(32, (3, 5, 5), strides=(1, 2, 2), use_bias=False))\n",
        "model_small.add(BatchNormalization(momentum=0))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "model_small.add(ZeroPadding3D(padding=(1, 2, 2)))\n",
        "model_small.add(Conv3D(64, (3, 5, 5), strides=(1, 1, 1), use_bias=False))\n",
        "model_small.add(BatchNormalization(momentum=0))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "model_small.add(ZeroPadding3D(padding=(1, 1, 1)))\n",
        "model_small.add(Conv3D(96, (3, 3, 3), strides=(1, 2, 2), use_bias=False))\n",
        "model_small.add(BatchNormalization(momentum=0))\n",
        "model_small.add(Activation('relu'))\n",
        "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "model_small.add(TimeDistributed(Flatten()))\n",
        "\n",
        "model_small.add(Bidirectional(GRU(50)))\n",
        "model_small.add(Dropout(0.2))\n",
        "\n",
        "model_small.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adam2 = keras.optimizers.Adam(lr=0.001)\n",
        "sgd = optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
        "save_weights = keras.callbacks.ModelCheckpoint('model_small.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "model_small.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#model_small.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    os.remove('model_small.hdf5')\n",
        "except:\n",
        "    print('file is not there')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model_small.fit_generator(train_data, epochs=100, steps_per_epoch=139, \n",
        "                                    validation_data=val_data, validation_steps=31, callbacks=[save_weights], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "uumHCDh8QI2B"
      },
      "outputs": [],
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "colab_type": "code",
        "id": "YwjAhHs8QUhh",
        "outputId": "daffca35-84bb-40e5-d034-2d7de2b04f21"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ]
}