{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import shutil \n",
    "import math\n",
    "import numpy as np\n",
    "from albumentations.augmentations import transforms\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats as s\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, Activation, GRU\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Convolution2D, ZeroPadding2D, Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "#os.makedirs(base_dir)\n",
    "#os.makedirs(train_dir)\n",
    "#os.makedirs(val_dir)\n",
    "#os.makedirs(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 'Begin, Choose, Connection, Navigation, Next, Previous, Start, Stop, Hello, Web'\n",
    "classes = classes.split(', ')\n",
    "classes = classes[:6]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in classes:\n",
    "    train_vids_dir = os.path.join(train_dir, class_name)\n",
    "    val_vids_dir = os.path.join(val_dir, class_name)\n",
    "    test_vids_dir = os.path.join(test_dir, class_name)\n",
    "    \n",
    "    os.makedirs(train_vids_dir)\n",
    "    os.makedirs(val_vids_dir)\n",
    "    os.makedirs(test_vids_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_people = 'F01, F02, F04, F05, F06, F07, F08, F09, M01, M04'.split(', ')\n",
    "#train_people = 'F01, F02, F05, F06, F07, F09, M01, M04'.split(', ')\n",
    "val_people = 'F10, M07'.split(', ')\n",
    "test_people = 'F11, M08'.split(', ')\n",
    "print(train_people)\n",
    "print(val_people)\n",
    "print(test_people)\n",
    "\n",
    "classes_num = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
    "classes_num = classes_num[:6]\n",
    "print(classes_num)\n",
    "word_ids = ['0'+str(i) if i < 10 else str(i) for i in range(1, 11) ]\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dict = dict(zip(classes_num, classes))\n",
    "classes_dict['01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, figsize=(8, 8)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid(False)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.imshow(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for classi in classes_num[0]:\n",
    "    for person in tqdm(train_people):\n",
    "        for word_id in word_ids:\n",
    "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "                img = cv2.imread(f, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "                \n",
    "                for k, rect in enumerate(rects):\n",
    "                    x1 = rect.left()\n",
    "                    y1 = rect.top()\n",
    "                    x2 = rect.right()\n",
    "                    y2 = rect.bottom()\n",
    "                    \n",
    "                    offset_x = (100-(abs(x1-x2)))/2\n",
    "                    offset_y = (100-(abs(y1-y2)))/2\n",
    "                    \n",
    "                    img = img[int(y1-offset_y):int(y2+offset_y), int(x1-offset_x):int(x2+offset_x)]\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                #cv2.imwrite('data/train/' + classes_dict[classi] + '/' + classi + '_' \n",
    "                #            + person + '_' \n",
    "                #            + word_id + '_' \n",
    "                #            + 'frame' + f[29:-4] + '.jpg', img)\n",
    "                cv2.imwrite('data/train/' \n",
    "                            + classes_dict[classi] + '/' \n",
    "                            + classi  + '_' \n",
    "                            + person  + '_' \n",
    "                            + word_id + '_' \n",
    "                            + f[28:-4] + '.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(classi):\n",
    "    start_idx = (len('data/train/') + len(classi)+1)\n",
    "    return start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_idx('Web'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classi in classes:\n",
    "    files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    files_clean = [file_clean for file_clean in sorted(files)\n",
    "                   if 'vert_flip' not in file_clean\n",
    "                   and 'hor_flip' not in file_clean\n",
    "                   and 'rot_45' not in file_clean\n",
    "                   and 'rot_315' not in file_clean\n",
    "                   and 'rot_135' not in file_clean\n",
    "                   and 'rot_225' not in file_clean\n",
    "                   and 'rot_90' not in file_clean\n",
    "                   and 'rot_270' not in file_clean\n",
    "                   and 'rand_contr' not in file_clean\n",
    "                   and 'noised' not in file_clean]\n",
    "    files_rem = [file_rem for file_rem in sorted(files)\n",
    "                   if 'vert_flip' in file_rem\n",
    "                   or 'hor_flip' in file_rem\n",
    "                   or 'rot_45' in file_rem\n",
    "                   or 'rot_315' in file_rem\n",
    "                   or 'rot_135' in file_rem\n",
    "                   or 'rot_225' in file_rem\n",
    "                   or 'rot_90' in file_rem\n",
    "                   or 'rot_270' in file_rem\n",
    "                   or 'rand_contr' in file_rem\n",
    "                   or 'noised' in file_rem]\n",
    "    for file_to_rem in files_rem: os.remove(file_to_rem)\n",
    "    files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    print(classi, 'dir: removed', len(files_rem), 'files.', len(files), 'to be augmented')\n",
    "\n",
    "    for file_to_hor_flip in files_clean:\n",
    "        img = cv2.imread(file_to_hor_flip, 1)\n",
    "        img = transforms.HorizontalFlip().apply(img)\n",
    "        file_to_write = file_to_hor_flip[:get_idx(classi)] + '[hor_flip]' + file_to_hor_flip[get_idx(classi):]\n",
    "        cv2.imwrite(file_to_write, img)\n",
    "    files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    files_hor_flipped = [file_hor_flipped for file_hor_flipped in sorted(files) if 'hor_flip' in file_hor_flipped]\n",
    "    print('Horizontally flipped', len(files_hor_flipped), 'files.', len(files), 'in the directory')\n",
    "    \n",
    "    for file_to_vert_flip in files_clean:\n",
    "        img = cv2.imread(file_to_vert_flip, 1)\n",
    "        img = transforms.VerticalFlip().apply(img)\n",
    "        file_to_write = file_to_vert_flip[:get_idx(classi)] + '[vert_flip]' + file_to_vert_flip[get_idx(classi):]\n",
    "        cv2.imwrite(file_to_write, img)\n",
    "    files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    files_vert_flipped = [file_vert_flipped for file_vert_flipped in sorted(files) if 'vert_flip' in file_vert_flipped]\n",
    "    print('Vertically flipped', len(files_vert_flipped), 'files.', len(files), 'in the directory')\n",
    "    \n",
    "    #files_not_noised = [f for f in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg'))) if 'noised' not in f ]    \n",
    "    #for file_to_rand_contr in files_not_noised:\n",
    "    #    img = cv2.imread(file_to_rand_contr, 1)\n",
    "    #    img = transforms.RandomContrast().apply(img)\n",
    "    #    file_to_write = file_to_rand_contr[:get_idx(classi)] + '[rand_contr]' + file_to_rand_contr[get_idx(classi):]\n",
    "    #    #print(file_to_rand_contr)\n",
    "    #    #print(file_to_write)\n",
    "    #    cv2.imwrite(file_to_write, img)\n",
    "    #files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    #files_rand_contred = [file_rand_contred for file_rand_contred in sorted(files) if 'rand_contr' in file_rand_contred]\n",
    "    #print('Random contrasted', len(files_rand_contred), 'files.', len(files), 'in the directory')\n",
    "    \n",
    "    files_not_rand_contred = [file_not_rand_contred for file_not_rand_contred in sorted(files) if 'rand_contr' not in file_not_rand_contred]\n",
    "    for file_to_noise in files_not_rand_contred:\n",
    "        if '[hor_flip]' in file_to_noise:\n",
    "            img = cv2.imread(file_to_noise, 1)\n",
    "            gauss = np.random.uniform(0, 64, img.size)\n",
    "            gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
    "            img = cv2.add(img,gauss)\n",
    "            file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
    "            #print(file_to_noise)\n",
    "            #print(file_to_write)\n",
    "            cv2.imwrite(file_to_write, img)\n",
    "        elif '[vert_flip]' in file_to_noise:\n",
    "            img = cv2.imread(file_to_noise, 1)\n",
    "            gauss = np.random.uniform(0, 64, img.size)\n",
    "            gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
    "            img = cv2.add(img,gauss)\n",
    "            file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
    "            cv2.imwrite(file_to_write, img)\n",
    "        else:\n",
    "            img = cv2.imread(file_to_noise, 1)\n",
    "            gauss = np.random.uniform(0, 64, img.size)\n",
    "            gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
    "            img = cv2.add(img,gauss)\n",
    "            file_to_write = file_to_noise[:get_idx(classi)] + '[noised]' + file_to_noise[get_idx(classi):]\n",
    "            cv2.imwrite(file_to_write, img)\n",
    "    files = [file for file in sorted(glob.glob(os.path.join('data/train/' + classi + '/*.jpg')))]\n",
    "    files_noised = [file_noised for file_noised in sorted(files) if 'noised' in file_noised]\n",
    "    print('Noised', len(files_noised), 'files.', len(files), 'in the directory')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for classi in classes_num:\n",
    "    for person in tqdm(val_people):\n",
    "        for word_id in word_ids:\n",
    "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "            #for f in sorted(glob.glob(os.path.join('MIRACL/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "                img = cv2.imread(f, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "                \n",
    "                for k, rect in enumerate(rects):\n",
    "                    x1 = rect.left()\n",
    "                    y1 = rect.top()\n",
    "                    x2 = rect.right()\n",
    "                    y2 = rect.bottom()\n",
    "                    \n",
    "                    offset_x = (100-(abs(x1-x2)))/2\n",
    "                    offset_y = (100-(abs(y1-y2)))/2\n",
    "                    \n",
    "                    img = img[int(y1-offset_y):int(y2+offset_y), int(x1-offset_x):int(x2+offset_x)]\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                #cv2.imwrite('data/validation/' \n",
    "                #            + classes_dict[classi] + '/' \n",
    "                #            + classi + '_' \n",
    "                #            + person + '_' \n",
    "                #            + word_id + '_' \n",
    "                #            + 'frame' + f[29:-4] + '.jpg', img)\n",
    "                \n",
    "                cv2.imwrite('data/validation/' \n",
    "                            + classes_dict[classi] + '/' \n",
    "                            + classi  + '_' \n",
    "                            + person  + '_' \n",
    "                            + word_id + '_' \n",
    "                            + f[28:-4] + '.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "train_class = []\n",
    "\n",
    "for class_id in tqdm(range(len(classes))):\n",
    "    images = sorted(glob.glob('data/train/' + classes[class_id] + '/*.jpg'))\n",
    "    for i in range(len(images)):\n",
    "        if ('noised' in images[i] or 'rand_contr' in images[i]\n",
    "        or 'vert_flip' in images[i] or 'hor_flip' in images[i]):\n",
    "            train_image.append(images[i].split('/')[3])\n",
    "            #print(images[i].split('/')[3])\n",
    "            train_class.append(classes_dict[images[i].split('/')[3].split(']')[-1][:2]])\n",
    "            #print(images[i].split('/')[3].split(']')[-1][:2])\n",
    "        else:\n",
    "            #print(images[i])\n",
    "            train_image.append(images[i].split('/')[3])\n",
    "            #print(images[i].split('/')[3])\n",
    "            train_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
    "            #print(images[i].split('/')[3].split('_')[0])\n",
    "        \n",
    "train_data = pd.DataFrame()\n",
    "train_data['image'] = train_image\n",
    "train_data['class'] = train_class\n",
    "\n",
    "train_data.to_csv('data/miracl/train_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/miracl/train_new.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = train['class']\n",
    "y_tr_dummy = pd.get_dummies(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image = []\n",
    "val_class = []\n",
    "\n",
    "for class_id in tqdm(range(len(classes))):\n",
    "    images = sorted(glob.glob('data/validation/' + classes[class_id] + '/*.jpg'))\n",
    "    for i in range(len(images)):\n",
    "        val_image.append(images[i].split('/')[3])\n",
    "        val_class.append(classes_dict[images[i].split('/')[3].split('_')[0]])\n",
    "val_data = pd.DataFrame()\n",
    "val_data['image'] = val_image\n",
    "val_data['class'] = val_class\n",
    "\n",
    "val_data.to_csv('data/miracl/val_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('data/miracl/val_new.csv')\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = val['class']\n",
    "y_val_dummy = pd.get_dummies(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = MobileNet(weights='imagenet', include_top=False, input_shape=(100,100,3), pooling='avg')\n",
    "#conv_base = MobileNet(weights='imagenet', include_top=False, input_shape=(100,100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 64\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 1024)) #like in global_average_pooling2d_1\n",
    "    labels = np.zeros(shape=(sample_count, 6)) # 6 - num of classes\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(100, 100),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        sys.stdout.write('\\rProcessed: %d, size of features: %f' %((i * batch_size), sys.getsizeof(features)*1e-6))\n",
    "        sys.stdout.flush()\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO tqdm(range(len(....))\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features, validation_labels = extract_features(val_dir, len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape)\n",
    "print(validation_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(train_features)*1e-6)\n",
    "print(sys.getsizeof(validation_features)*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten them to (samples, 16384):\n",
    "# train_features = np.reshape(train_features, (41742, 3 * 3 * 1024))\n",
    "# validation_features = np.reshape(validation_features, (1746, 3 * 3 * 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape)\n",
    "print(validation_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_rnn = np.reshape(train_features, (train_features.shape[0], 1, train_features.shape[1]))\n",
    "\n",
    "\n",
    "validation_features_rnn = np.reshape(validation_features, (validation_features.shape[0], 1, validation_features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features_rnn.shape)\n",
    "print(validation_features_rnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(1, 1024)))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#model.add(LSTM(256, input_shape=(1, 9216)))\n",
    "#model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fc = Sequential()\n",
    "#model_fc.add(Dense(128, activation='relu', input_dim=3 * 3 * 1024))\n",
    "model_fc.add(Dense(128, activation='relu', input_dim=1024))\n",
    "model_fc.add(Dropout(0.3))\n",
    "model_fc.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "adam2 = optimizers.Adam(lr=0.001)\n",
    "save_weights = ModelCheckpoint('weight_fc.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "model_fc.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
    "model_fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "adam2 = optimizers.Adam(lr=0.001)\n",
    "save_weights = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fc = model_fc.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[save_weights],\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features_rnn, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[save_weights],\n",
    "                    validation_data=(validation_features_rnn, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weight.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for classi in classes_num:\n",
    "    for person in tqdm(test_people):\n",
    "        for word_id in word_ids:\n",
    "            #for f in sorted(glob.glob(os.path.join('MIRACL/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "            for f in sorted(glob.glob(os.path.join('data/miracl/'+person+'/words/'+classi+'/'+ word_id, \"*.jpg\"))):\n",
    "                img = cv2.imread(f, 1)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "                \n",
    "                for k, rect in enumerate(rects):\n",
    "                    x1 = rect.left()\n",
    "                    y1 = rect.top()\n",
    "                    x2 = rect.right()\n",
    "                    y2 = rect.bottom()\n",
    "                    \n",
    "                    offset_x = (128-(abs(x1-x2)))/2\n",
    "                    offset_y = (128-(abs(y1-y2)))/2\n",
    "                    \n",
    "                    img = img[int(y1-offset_y):int(y2+offset_y), int(x1-offset_x):int(x2+offset_x)]\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                cv2.imwrite('data/test_miracl/' + classi + '_' \n",
    "                            + person + '_' \n",
    "                            + word_id + '_' \n",
    "                            + 'frame' + f[29:-4] + '.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(glob.glob(\"data/test_miracl/*.jpg\"))\n",
    "test_image = []\n",
    "test_class = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    #print(images[i])\n",
    "    test_image.append(sorted(images)[i].split('/')[2])\n",
    "    test_class.append(sorted(images)[i].split('/')[2][:2])\n",
    "\n",
    "\n",
    "test_data = pd.DataFrame()\n",
    "test_data['image'] = test_image\n",
    "test_data['class'] = test_class\n",
    "test_images = test_data['image']\n",
    "\n",
    "test_data.to_csv('test_new.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_new.csv')\n",
    "y_test = test['class']\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = sorted(list(set([test_images[i].split('.')[0][0:9] for i in range(len(test_images))])))\n",
    "test_videos = pd.DataFrame(video_names)\n",
    "\n",
    "test_videos.columns = ['dummy']\n",
    "test_videos = test_videos['dummy']\n",
    "\n",
    "print(test_videos.shape[0])\n",
    "test_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape[0])\n",
    "test_images.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "actual = []\n",
    "\n",
    "for video_id in tqdm(range(test_videos.shape[0])):\n",
    "    videoFile = test_videos[video_id]\n",
    "    \n",
    "    files = glob.glob('temp/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    \n",
    "    for frame_id in range(test_images.shape[0]):\n",
    "        frame = test_images[frame_id]\n",
    "        if test_images[frame_id].split('.')[0][0:9] == videoFile:\n",
    "            img = shutil.copyfile('data/test_miracl/'+test_images[frame_id], 'temp/'+test_images[frame_id]) \n",
    "    \n",
    "    images = glob.glob(\"temp/*.jpg\")\n",
    "    \n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(100,100,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = keras.applications.mobilenet.preprocess_input(img)\n",
    "        \n",
    "        prediction_images.append(img)\n",
    "        \n",
    "    prediction_images = np.array(prediction_images)\n",
    "    \n",
    "    prediction_images = conv_base.predict(prediction_images)\n",
    "    \n",
    "    #prediction_images = prediction_images.reshape(prediction_images.shape[0], 1024)\n",
    "    \n",
    "    #prediction_images = np.reshape(prediction_images, \n",
    "    #                               (prediction_images.shape[0], 1, prediction_images.shape[1])) #(x, 1, 1024)\n",
    "    \n",
    "    prediction = model_fc.predict_classes(prediction_images)\n",
    "    predict.append(y_test.columns.values[s.mode(prediction)[0][0]])\n",
    "    \n",
    "    actual.append(int(videoFile.split('_')[0].lstrip('0')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
