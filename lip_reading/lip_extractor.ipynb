{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import shutil \n",
    "import math\n",
    "import numpy as np\n",
    "from albumentations.augmentations import transforms\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats as s\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "'''import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, Activation, GRU, Input, Conv3D, MaxPooling3D, GlobalMaxPooling3D, ZeroPadding3D, AveragePooling3D\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Convolution2D, ZeroPadding2D, Bidirectional, TimeDistributed, GlobalAveragePooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.callbacks import ModelCheckpoint'''\n",
    "\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#from keras.regularizers import l2\n",
    "#from keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train people ['F01', 'F02', 'F04', 'F05', 'F06', 'F07', 'F08', 'F09', 'M01', 'M02', 'M04']\n",
    "\n",
    "from utils import mouth_extractor, get_distribution, frames_padding, data_augmenter\n",
    "train_people, val_people, test_people, classes_num, classes_dict, word_ids = mouth_extractor.split_speakers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:01<00:00, 48.11s/it]\n",
      "100%|██████████| 10/10 [01:15<00:00,  7.57s/it]\n"
     ]
    }
   ],
   "source": [
    "mouth_extractor.create_dirs()\n",
    "\n",
    "#mouth_extractor.extract_train(2, 2, 2)\n",
    "#mouth_extractor.extract_val(2, 2, 2)\n",
    "\n",
    "mouth_extractor.extract_train()\n",
    "mouth_extractor.extract_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, \\\n",
    "video_names, \\\n",
    "frame_nums, \\\n",
    "video_names_uniq, \\\n",
    "frame_nums_uniq, \\\n",
    "vids_and_frames, \\\n",
    "frames_distribution = get_distribution.get_frames_distribution_train()\n",
    "\n",
    "all_images_val, \\\n",
    "video_names_val, \\\n",
    "frame_nums_val, \\\n",
    "video_names_uniq_val, \\\n",
    "frame_nums_uniq_val, \\\n",
    "vids_and_frames_val, \\\n",
    "frames_distribution_val = get_distribution.get_frames_distribution_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train max: 22\n",
      "Train min: 5\n",
      "Train mean: 10.952\n",
      "\n",
      "Val max: 13\n",
      "Val min: 5\n",
      "Val mean: 8.73\n"
     ]
    }
   ],
   "source": [
    "print('Train max:', int(max(vids_and_frames.values())))\n",
    "print('Train min:', int(min(vids_and_frames.values())))\n",
    "print('Train mean:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Val max:', int(max(vids_and_frames_val.values())))\n",
    "print('Val min:', int(min(vids_and_frames_val.values())))\n",
    "print('Val mean:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/Documents/SpeechRecognition/lip_reading/utils/get_distribution.py:110: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_distribution.plot_distribution(frames_distribution, frames_distribution_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Removed 510 ( 510 in this dir)\n",
      "Choose Removed 810 ( 300 in this dir)\n",
      "Connection Removed 1406 ( 596 in this dir)\n",
      "Navigation Removed 2237 ( 831 in this dir)\n",
      "Next Removed 2593 ( 356 in this dir)\n",
      "Previous Removed 3031 ( 438 in this dir)\n",
      "Start Removed 3260 ( 229 in this dir)\n",
      "Stop Removed 3615 ( 355 in this dir)\n",
      "Hello Removed 4223 ( 608 in this dir)\n",
      "Web Removed 4475 ( 252 in this dir)\n",
      "\n",
      "Removed 349 videos\n"
     ]
    }
   ],
   "source": [
    "comp, abnormal_vids = frames_padding.remove_short_long_train(all_images, video_names, frame_nums, video_names_uniq, frame_nums_uniq, vids_and_frames, frames_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Removed 35 ( 35 in this dir)\n",
      "Choose Removed 47 ( 12 in this dir)\n",
      "Connection Removed 169 ( 122 in this dir)\n",
      "Navigation Removed 320 ( 151 in this dir)\n",
      "Next Removed 349 ( 29 in this dir)\n",
      "Previous Removed 404 ( 55 in this dir)\n",
      "Start Removed 404 ( 0 in this dir)\n",
      "Stop Removed 410 ( 6 in this dir)\n",
      "Hello Removed 427 ( 17 in this dir)\n",
      "Web Removed 466 ( 39 in this dir)\n",
      "\n",
      "Removed 49 videos\n"
     ]
    }
   ],
   "source": [
    "comp_val, abnormal_vids_val = frames_padding.remove_short_long_val(all_images_val, video_names_val, frame_nums_val, video_names_uniq_val, frame_nums_uniq_val, vids_and_frames_val, frames_distribution_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, \\\n",
    "video_names, \\\n",
    "frame_nums, \\\n",
    "video_names_uniq, \\\n",
    "frame_nums_uniq, \\\n",
    "vids_and_frames, \\\n",
    "frames_distribution = get_distribution.get_frames_distribution_train()\n",
    "\n",
    "all_images_val, \\\n",
    "video_names_val, \\\n",
    "frame_nums_val, \\\n",
    "video_names_uniq_val, \\\n",
    "frame_nums_uniq_val, \\\n",
    "vids_and_frames_val, \\\n",
    "frames_distribution_val = get_distribution.get_frames_distribution_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train max: 12\n",
      "Train min: 8\n",
      "Train mean: 9.949308755760368\n",
      "\n",
      "Val max: 10\n",
      "Val min: 7\n",
      "Val mean: 8.47682119205298\n"
     ]
    }
   ],
   "source": [
    "print('Train max:', int(max(vids_and_frames.values())))\n",
    "print('Train min:', int(min(vids_and_frames.values())))\n",
    "print('Train mean:', (np.array(list(vids_and_frames.values())).astype(np.float).mean()))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Val max:', int(max(vids_and_frames_val.values())))\n",
    "print('Val min:', int(min(vids_and_frames_val.values())))\n",
    "print('Val mean:', (np.array(list(vids_and_frames_val.values())).astype(np.float).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6477"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame_num = int(max(vids_and_frames.values())) #12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_padding.pad_train(target_frame_num, classes_dict, vids_and_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696\n",
      "1608\n",
      "2280\n",
      "2844\n",
      "3588\n",
      "4440\n",
      "5424\n",
      "6276\n",
      "6936\n",
      "7812\n"
     ]
    }
   ],
   "source": [
    "all_images_padded = []\n",
    "\n",
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/train/' + classi + '/*.jpg')):\n",
    "        all_images_padded.append(i)\n",
    "    print(len(all_images_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frame_num_val = int(max(vids_and_frames_val.values())) #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_padding.pad_val(target_frame_num_val, classes_dict, vids_and_frames_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "340\n",
      "440\n",
      "510\n",
      "660\n",
      "810\n",
      "1010\n",
      "1200\n",
      "1380\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "all_images_padded_val = []\n",
    "\n",
    "for classi in classes_dict.values():\n",
    "    for i in sorted(glob.glob('data/validation/' + classi + '/*.jpg')):\n",
    "        all_images_padded_val.append(i)\n",
    "    print(len(all_images_padded_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 'Begin, Choose, Connection, Navigation, Next, Previous, Start, Stop, Hello, Web'\n",
    "classes = classes.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin dir: removed 0 files. 696 to be augmented\n",
      "Horizontally flipped 696 files. 1392 in the directory\n",
      "Vertically flipped 696 files. 2088 in the directory\n",
      "Noised 2088 files. 4176 in the directory\n",
      "\n",
      "Choose dir: removed 0 files. 912 to be augmented\n",
      "Horizontally flipped 912 files. 1824 in the directory\n",
      "Vertically flipped 912 files. 2736 in the directory\n",
      "Noised 2736 files. 5472 in the directory\n",
      "\n",
      "Connection dir: removed 0 files. 672 to be augmented\n",
      "Horizontally flipped 672 files. 1344 in the directory\n",
      "Vertically flipped 672 files. 2016 in the directory\n",
      "Noised 2016 files. 4032 in the directory\n",
      "\n",
      "Navigation dir: removed 0 files. 564 to be augmented\n",
      "Horizontally flipped 564 files. 1128 in the directory\n",
      "Vertically flipped 564 files. 1692 in the directory\n",
      "Noised 1692 files. 3384 in the directory\n",
      "\n",
      "Next dir: removed 0 files. 744 to be augmented\n",
      "Horizontally flipped 744 files. 1488 in the directory\n",
      "Vertically flipped 744 files. 2232 in the directory\n",
      "Noised 2232 files. 4464 in the directory\n",
      "\n",
      "Previous dir: removed 0 files. 852 to be augmented\n",
      "Horizontally flipped 852 files. 1704 in the directory\n",
      "Vertically flipped 852 files. 2556 in the directory\n",
      "Noised 2556 files. 5112 in the directory\n",
      "\n",
      "Start dir: removed 0 files. 984 to be augmented\n",
      "Horizontally flipped 984 files. 1968 in the directory\n",
      "Vertically flipped 984 files. 2952 in the directory\n",
      "Noised 2952 files. 5904 in the directory\n",
      "\n",
      "Stop dir: removed 0 files. 852 to be augmented\n",
      "Horizontally flipped 852 files. 1704 in the directory\n",
      "Vertically flipped 852 files. 2556 in the directory\n",
      "Noised 2556 files. 5112 in the directory\n",
      "\n",
      "Hello dir: removed 0 files. 660 to be augmented\n",
      "Horizontally flipped 660 files. 1320 in the directory\n",
      "Vertically flipped 660 files. 1980 in the directory\n",
      "Noised 1980 files. 3960 in the directory\n",
      "\n",
      "Web dir: removed 0 files. 876 to be augmented\n",
      "Horizontally flipped 876 files. 1752 in the directory\n",
      "Vertically flipped 876 files. 2628 in the directory\n",
      "Noised 2628 files. 5256 in the directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_augmenter.augment_frames(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten, Activation, GRU, Input, Conv3D, MaxPooling3D, GlobalMaxPooling3D, ZeroPadding3D, AveragePooling3D\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Convolution2D, ZeroPadding2D, Bidirectional, TimeDistributed, GlobalAveragePooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-11 20:50:08--  https://gist.githubusercontent.com/Emadeldeen-24/736c33ac2af0c00cc48810ad62e1f54a/raw/f2428c9ba7c13e08e2aa9b2d5ff64ad575a99dd8/tweaked_ImageGenerator_v2.py\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.76.133\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.76.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45797 (45K) [text/plain]\n",
      "Saving to: ‘tweaked_ImageGenerator_v2.py.2’\n",
      "\n",
      "tweaked_ImageGenera 100%[===================>]  44.72K   108KB/s    in 0.4s    \n",
      "\n",
      "2020-02-11 20:50:10 (108 KB/s) - ‘tweaked_ImageGenerator_v2.py.2’ saved [45797/45797]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://gist.githubusercontent.com/Emadeldeen-24/736c33ac2af0c00cc48810ad62e1f54a/raw/f2428c9ba7c13e08e2aa9b2d5ff64ad575a99dd8/tweaked_ImageGenerator_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweaked_ImageGenerator_v2 import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46872 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator()\n",
    "train_data = datagen.flow_from_directory('data/train', target_size=(50, 100), batch_size=2, frames_per_step=22, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1510 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = datagen.flow_from_directory('data/validation', target_size=(50, 100), batch_size=2, frames_per_step=22, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.ceil(51120/(2*22)))\n",
    "print(math.ceil((1510/(2*22))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to capture an EagerTensor without building a function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1a1578394272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#model_small.add(Bidirectional(LSTM(256, activation='relu', return_sequences=True)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel_small\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_small\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1940\u001b[0m                                         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                                         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1942\u001b[0;31m                                         constraint=self.bias_constraint)\n\u001b[0m\u001b[1;32m   1943\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                             constraint=constraint)\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight_regularizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \"\"\"\n\u001b[1;32m    619\u001b[0m     v = tf_keras_backend.variable(\n\u001b[0;32m--> 620\u001b[0;31m         value, dtype=dtype, name=name, constraint=constraint)\n\u001b[0m\u001b[1;32m    621\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tocoo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(value, dtype, name, constraint)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m    815\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    233\u001b[0m                         shape=None):\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[1;32m   1542\u001b[0m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m                 name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m   1544\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1278\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         raise RuntimeError(\"Attempting to capture an EagerTensor without \"\n\u001b[0m\u001b[1;32m   1281\u001b[0m                            \"building a function.\")\n\u001b[1;32m   1282\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to capture an EagerTensor without building a function."
     ]
    }
   ],
   "source": [
    "model_small = Sequential()\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 2, 2), input_shape=(22, 50, 100, 3)))\n",
    "model_small.add(Conv3D(32, (3, 5, 5), strides=(1, 2, 2), activation='relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model_small.add(Dropout(.3))\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 2, 2)))\n",
    "model_small.add(Conv3D(64, (3, 5, 5), strides=(1, 1, 1), activation='relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model_small.add(Dropout(.3))\n",
    "\n",
    "model_small.add(ZeroPadding3D(padding=(1, 1, 1)))\n",
    "model_small.add(Conv3D(96, (3, 3, 3), strides=(1, 2, 2), activation='relu'))\n",
    "model_small.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
    "model_small.add(Dropout(.3))\n",
    "\n",
    "model_small.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#model_small.add(Bidirectional(LSTM(256, activation='relu', return_sequences=True)))\n",
    "model_small.add(Bidirectional(LSTM(256, activation='relu', return_sequences=False)))\n",
    "\n",
    "model_small.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding3d_1 (ZeroPaddin (None, 24, 54, 104, 3)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 22, 25, 50, 32)    7232      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 22, 12, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 12, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "zero_padding3d_2 (ZeroPaddin (None, 24, 16, 29, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 22, 12, 25, 64)    153664    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 22, 6, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 6, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "zero_padding3d_3 (ZeroPaddin (None, 24, 8, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 22, 3, 6, 96)      165984    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 22, 1, 3, 96)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 22, 1, 3, 96)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 22, 288)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 22, 512)           1116160   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,023,082\n",
      "Trainable params: 3,023,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "adam2 = optimizers.Adam(lr=0.0001)\n",
    "save_weights = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "model_small.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
    "model_small.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   4/1162 [..............................] - ETA: 5:09:17 - loss: 6.7499 - accuracy: 0.9716"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cb39da2cb967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model_small.fit_generator(train_data, epochs=20, steps_per_epoch=1162, \n\u001b[0;32m----> 2\u001b[0;31m                               validation_data=val_data, validation_steps=35, shuffle=False)\n\u001b[0m",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/pythonenvs/lr13.6/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model_small.fit_generator(train_data, epochs=20, steps_per_epoch=1162, \n",
    "                              validation_data=val_data, validation_steps=35, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/pythonenvs/lr13.6/lib/python3.6/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/pythonenvs/lr13.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"gl...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "video = Input(shape=(22, 50, 100, 3))\n",
    "\n",
    "cnn_base = MobileNetV2(input_shape=(50, 100, 3), include_top=False)\n",
    "cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n",
    "\n",
    "cnn = Model(input=cnn_base.input, output=cnn_out)\n",
    "encoded_frames = TimeDistributed(cnn)(video)\n",
    "\n",
    "encoded_sequence = GRU(66)(encoded_frames)\n",
    "outputs = Dense(10, activation=\"softmax\")(encoded_sequence)\n",
    "\n",
    "model = Model([video], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam2 = optimizers.Adam(lr=0.0001)\n",
    "save_weights = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam2,metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
